2025-07-27 11:42:09,716 - INFO - Using GPU: 0
2025-07-27 11:42:09,716 - INFO - Starting Training Mode...
2025-07-27 11:42:09,967 - INFO - Loaded real volume: torch.Size([1, 1, 16, 32, 32])
2025-07-27 11:42:09,998 - INFO - Created pyramid with 8 scales.
2025-07-27 11:42:09,998 - INFO -   Scale 7 (Coarsest=True): torch.Size([1, 1, 6, 11, 11])
2025-07-27 11:42:09,999 - INFO -   Scale 6 (Coarsest=False): torch.Size([1, 1, 7, 13, 13])
2025-07-27 11:42:09,999 - INFO -   Scale 5 (Coarsest=False): torch.Size([1, 1, 8, 15, 15])
2025-07-27 11:42:09,999 - INFO -   Scale 4 (Coarsest=False): torch.Size([1, 1, 9, 17, 17])
2025-07-27 11:42:10,000 - INFO -   Scale 3 (Coarsest=False): torch.Size([1, 1, 10, 20, 20])
2025-07-27 11:42:10,000 - INFO -   Scale 2 (Coarsest=False): torch.Size([1, 1, 12, 23, 23])
2025-07-27 11:42:10,000 - INFO -   Scale 1 (Coarsest=False): torch.Size([1, 1, 14, 27, 27])
2025-07-27 11:42:10,000 - INFO -   Scale 0 (Coarsest=False): torch.Size([1, 1, 16, 32, 32])
2025-07-27 11:42:10,002 - INFO - 
--- Training Scale 7 ---
2025-07-27 11:42:10,002 - INFO - Volume size: torch.Size([6, 11, 11])
2025-07-27 11:42:12,697 - INFO - Scale [7] Iter [0/100000] Loss_D: 242.0128 Loss_G: 4.6550 (Adv: 0.1783 Rec: 4.4766)
2025-07-27 11:42:47,261 - INFO - Scale [7] Iter [1000/100000] Loss_D: 0.0023 Loss_G: 0.0255 (Adv: 0.0249 Rec: 0.0006)
2025-07-27 11:43:20,856 - INFO - Scale [7] Iter [2000/100000] Loss_D: -0.0089 Loss_G: -0.0712 (Adv: -0.0738 Rec: 0.0025)
2025-07-27 11:43:54,444 - INFO - Scale [7] Iter [3000/100000] Loss_D: -0.0025 Loss_G: -0.0560 (Adv: -0.0567 Rec: 0.0007)
2025-07-27 11:44:27,857 - INFO - Scale [7] Iter [4000/100000] Loss_D: 3.6332 Loss_G: -0.0232 (Adv: -0.0804 Rec: 0.0571)
2025-07-27 11:45:02,322 - INFO - Scale [7] Iter [5000/100000] Loss_D: -0.0058 Loss_G: -0.1647 (Adv: -0.1662 Rec: 0.0015)
2025-07-27 11:45:35,672 - INFO - Scale [7] Iter [6000/100000] Loss_D: -0.0044 Loss_G: -0.1620 (Adv: -0.1635 Rec: 0.0015)
2025-07-27 11:46:09,864 - INFO - Scale [7] Iter [7000/100000] Loss_D: -0.0038 Loss_G: -0.1362 (Adv: -0.1374 Rec: 0.0012)
2025-07-27 11:46:43,800 - INFO - Scale [7] Iter [8000/100000] Loss_D: 0.0247 Loss_G: -0.8270 (Adv: -0.8273 Rec: 0.0003)
2025-07-27 11:47:17,946 - INFO - Scale [7] Iter [9000/100000] Loss_D: -0.0006 Loss_G: 0.1868 (Adv: 0.1858 Rec: 0.0010)
2025-07-27 11:47:51,242 - INFO - Scale [7] Iter [10000/100000] Loss_D: 0.0083 Loss_G: 0.4641 (Adv: 0.4630 Rec: 0.0011)
2025-07-27 11:48:24,889 - INFO - Scale [7] Iter [11000/100000] Loss_D: 0.0003 Loss_G: 0.3963 (Adv: 0.3880 Rec: 0.0084)
2025-07-27 11:48:59,214 - INFO - Scale [7] Iter [12000/100000] Loss_D: 0.0410 Loss_G: 0.4131 (Adv: 0.4127 Rec: 0.0003)
2025-07-27 11:49:33,502 - INFO - Scale [7] Iter [13000/100000] Loss_D: 0.0129 Loss_G: 1.4314 (Adv: 1.4312 Rec: 0.0002)
2025-07-27 11:50:06,510 - INFO - Scale [7] Iter [14000/100000] Loss_D: -0.0003 Loss_G: 0.1976 (Adv: 0.1973 Rec: 0.0003)
2025-07-27 11:50:40,172 - INFO - Scale [7] Iter [15000/100000] Loss_D: 0.0013 Loss_G: 0.1831 (Adv: 0.1822 Rec: 0.0009)
2025-07-27 11:51:14,223 - INFO - Scale [7] Iter [16000/100000] Loss_D: 0.0076 Loss_G: 0.0325 (Adv: 0.0305 Rec: 0.0020)
2025-07-27 11:51:48,562 - INFO - Scale [7] Iter [17000/100000] Loss_D: 0.0094 Loss_G: 1.3873 (Adv: 1.3872 Rec: 0.0001)
2025-07-27 11:52:22,459 - INFO - Scale [7] Iter [18000/100000] Loss_D: 0.0013 Loss_G: 1.2572 (Adv: 1.2570 Rec: 0.0002)
2025-07-27 11:52:56,885 - INFO - Scale [7] Iter [19000/100000] Loss_D: 0.0006 Loss_G: 1.4556 (Adv: 1.4554 Rec: 0.0002)
2025-07-27 11:53:30,441 - INFO - Scale [7] Iter [20000/100000] Loss_D: -0.0009 Loss_G: 1.3507 (Adv: 1.3502 Rec: 0.0004)
2025-07-27 11:54:04,508 - INFO - Scale [7] Iter [21000/100000] Loss_D: 0.0191 Loss_G: 1.1639 (Adv: 1.1635 Rec: 0.0004)
2025-07-27 11:54:38,012 - INFO - Scale [7] Iter [22000/100000] Loss_D: 0.0032 Loss_G: 0.1284 (Adv: 0.1283 Rec: 0.0001)
2025-07-27 11:55:12,753 - INFO - Scale [7] Iter [23000/100000] Loss_D: 0.0024 Loss_G: -0.0397 (Adv: -0.0402 Rec: 0.0005)
2025-07-27 11:55:46,689 - INFO - Scale [7] Iter [24000/100000] Loss_D: 0.0071 Loss_G: -0.3061 (Adv: -0.3068 Rec: 0.0007)
2025-07-27 11:56:20,972 - INFO - Scale [7] Iter [25000/100000] Loss_D: 0.0066 Loss_G: -0.4238 (Adv: -0.4246 Rec: 0.0007)
2025-07-27 11:56:54,766 - INFO - Scale [7] Iter [26000/100000] Loss_D: -0.0001 Loss_G: -0.5000 (Adv: -0.5002 Rec: 0.0002)
2025-07-27 11:57:28,056 - INFO - Scale [7] Iter [27000/100000] Loss_D: 0.0080 Loss_G: -4.1229 (Adv: -4.1260 Rec: 0.0031)
2025-07-27 11:58:02,476 - INFO - Scale [7] Iter [28000/100000] Loss_D: 0.0029 Loss_G: -3.8819 (Adv: -3.8825 Rec: 0.0007)
2025-07-27 11:58:36,398 - INFO - Scale [7] Iter [29000/100000] Loss_D: 0.0017 Loss_G: -4.1517 (Adv: -4.1523 Rec: 0.0006)
2025-07-27 11:59:11,271 - INFO - Scale [7] Iter [30000/100000] Loss_D: 0.0026 Loss_G: -4.0248 (Adv: -4.0251 Rec: 0.0003)
2025-07-27 11:59:45,662 - INFO - Scale [7] Iter [31000/100000] Loss_D: 0.0008 Loss_G: -4.4523 (Adv: -4.4526 Rec: 0.0003)
2025-07-27 12:00:17,300 - INFO - Scale [7] Iter [32000/100000] Loss_D: 0.0007 Loss_G: -4.4308 (Adv: -4.4310 Rec: 0.0002)
2025-07-27 12:00:46,627 - INFO - Scale [7] Iter [33000/100000] Loss_D: -0.0001 Loss_G: -4.1492 (Adv: -4.1497 Rec: 0.0005)
2025-07-27 12:01:16,514 - INFO - Scale [7] Iter [34000/100000] Loss_D: -0.0000 Loss_G: -4.1715 (Adv: -4.1719 Rec: 0.0003)
2025-07-27 12:01:48,384 - INFO - Scale [7] Iter [35000/100000] Loss_D: 0.0005 Loss_G: -4.1596 (Adv: -4.1605 Rec: 0.0009)
2025-07-27 12:02:19,676 - INFO - Scale [7] Iter [36000/100000] Loss_D: -0.0002 Loss_G: -4.2963 (Adv: -4.2971 Rec: 0.0008)
2025-07-27 12:02:50,628 - INFO - Scale [7] Iter [37000/100000] Loss_D: 0.0002 Loss_G: -4.4064 (Adv: -4.4069 Rec: 0.0005)
2025-07-27 12:03:21,598 - INFO - Scale [7] Iter [38000/100000] Loss_D: 0.0001 Loss_G: -3.6148 (Adv: -3.6152 Rec: 0.0004)
2025-07-27 12:03:56,123 - INFO - Scale [7] Iter [39000/100000] Loss_D: 0.0001 Loss_G: -3.8564 (Adv: -3.8567 Rec: 0.0003)
2025-07-27 12:04:30,203 - INFO - Scale [7] Iter [40000/100000] Loss_D: -0.0002 Loss_G: -3.5704 (Adv: -3.5714 Rec: 0.0010)
2025-07-27 12:05:04,403 - INFO - Scale [7] Iter [41000/100000] Loss_D: 0.0019 Loss_G: -0.9770 (Adv: -0.9771 Rec: 0.0001)
2025-07-27 12:05:38,071 - INFO - Scale [7] Iter [42000/100000] Loss_D: 0.0002 Loss_G: -0.3693 (Adv: -0.3696 Rec: 0.0003)
2025-07-27 12:06:12,741 - INFO - Scale [7] Iter [43000/100000] Loss_D: 0.0008 Loss_G: 0.0425 (Adv: 0.0420 Rec: 0.0006)
2025-07-27 12:06:46,438 - INFO - Scale [7] Iter [44000/100000] Loss_D: 0.0001 Loss_G: 0.3591 (Adv: 0.3588 Rec: 0.0002)
2025-07-27 12:07:20,207 - INFO - Scale [7] Iter [45000/100000] Loss_D: -0.0001 Loss_G: 0.1805 (Adv: 0.1804 Rec: 0.0001)
2025-07-27 12:07:54,613 - INFO - Scale [7] Iter [46000/100000] Loss_D: 0.0002 Loss_G: 0.0056 (Adv: 0.0054 Rec: 0.0001)
2025-07-27 12:08:27,772 - INFO - Scale [7] Iter [47000/100000] Loss_D: -0.0001 Loss_G: 0.1463 (Adv: 0.1460 Rec: 0.0003)
2025-07-27 12:09:02,338 - INFO - Scale [7] Iter [48000/100000] Loss_D: 0.0007 Loss_G: 0.4318 (Adv: 0.4315 Rec: 0.0003)
2025-07-27 12:09:36,232 - INFO - Scale [7] Iter [49000/100000] Loss_D: -0.0001 Loss_G: 0.3785 (Adv: 0.3779 Rec: 0.0006)
2025-07-27 12:10:11,234 - INFO - Scale [7] Iter [50000/100000] Loss_D: -0.0002 Loss_G: -0.0401 (Adv: -0.0403 Rec: 0.0002)
2025-07-27 12:10:45,351 - INFO - Scale [7] Iter [51000/100000] Loss_D: -0.0002 Loss_G: 0.1411 (Adv: 0.1411 Rec: 0.0000)
2025-07-27 12:11:19,082 - INFO - Scale [7] Iter [52000/100000] Loss_D: -0.0004 Loss_G: 0.0597 (Adv: 0.0597 Rec: 0.0000)
2025-07-27 12:11:53,148 - INFO - Scale [7] Iter [53000/100000] Loss_D: -0.0003 Loss_G: 0.1010 (Adv: 0.1010 Rec: 0.0000)
2025-07-27 12:12:27,641 - INFO - Scale [7] Iter [54000/100000] Loss_D: -0.0000 Loss_G: -0.1130 (Adv: -0.1130 Rec: 0.0000)
2025-07-27 12:13:01,390 - INFO - Scale [7] Iter [55000/100000] Loss_D: -0.0003 Loss_G: -0.0222 (Adv: -0.0222 Rec: 0.0000)
2025-07-27 12:13:34,324 - INFO - Scale [7] Iter [56000/100000] Loss_D: -0.0002 Loss_G: -0.2282 (Adv: -0.2282 Rec: 0.0000)
2025-07-27 12:14:07,735 - INFO - Scale [7] Iter [57000/100000] Loss_D: -0.0002 Loss_G: -0.2123 (Adv: -0.2123 Rec: 0.0000)
2025-07-27 12:14:42,495 - INFO - Scale [7] Iter [58000/100000] Loss_D: -0.0002 Loss_G: -0.1444 (Adv: -0.1445 Rec: 0.0000)
2025-07-27 12:15:17,001 - INFO - Scale [7] Iter [59000/100000] Loss_D: -0.0001 Loss_G: -0.1236 (Adv: -0.1236 Rec: 0.0000)
2025-07-27 12:15:51,646 - INFO - Scale [7] Iter [60000/100000] Loss_D: 0.0003 Loss_G: -0.1310 (Adv: -0.1310 Rec: 0.0000)
2025-07-27 12:16:26,281 - INFO - Scale [7] Iter [61000/100000] Loss_D: -0.0001 Loss_G: -0.0528 (Adv: -0.0528 Rec: 0.0000)
2025-07-27 12:17:00,672 - INFO - Scale [7] Iter [62000/100000] Loss_D: -0.0001 Loss_G: -0.1680 (Adv: -0.1680 Rec: 0.0000)
2025-07-27 12:17:35,077 - INFO - Scale [7] Iter [63000/100000] Loss_D: -0.0001 Loss_G: -0.2089 (Adv: -0.2089 Rec: 0.0000)
2025-07-27 12:18:09,510 - INFO - Scale [7] Iter [64000/100000] Loss_D: -0.0002 Loss_G: -0.0419 (Adv: -0.0420 Rec: 0.0000)
2025-07-27 12:18:43,475 - INFO - Scale [7] Iter [65000/100000] Loss_D: -0.0001 Loss_G: -0.0611 (Adv: -0.0611 Rec: 0.0000)
2025-07-27 12:19:17,786 - INFO - Scale [7] Iter [66000/100000] Loss_D: 0.0002 Loss_G: -0.0877 (Adv: -0.0877 Rec: 0.0000)
2025-07-27 12:19:52,572 - INFO - Scale [7] Iter [67000/100000] Loss_D: 0.0000 Loss_G: -0.3307 (Adv: -0.3307 Rec: 0.0000)
2025-07-27 12:20:26,568 - INFO - Scale [7] Iter [68000/100000] Loss_D: 0.0001 Loss_G: -0.3599 (Adv: -0.3600 Rec: 0.0000)
2025-07-27 12:21:01,469 - INFO - Scale [7] Iter [69000/100000] Loss_D: -0.0002 Loss_G: -0.2600 (Adv: -0.2601 Rec: 0.0000)
2025-07-27 12:21:36,172 - INFO - Scale [7] Iter [70000/100000] Loss_D: -0.0002 Loss_G: -0.4254 (Adv: -0.4254 Rec: 0.0000)
2025-07-27 12:22:10,519 - INFO - Scale [7] Iter [71000/100000] Loss_D: -0.0001 Loss_G: -0.3275 (Adv: -0.3275 Rec: 0.0000)
2025-07-27 12:22:43,736 - INFO - Scale [7] Iter [72000/100000] Loss_D: -0.0001 Loss_G: -0.1683 (Adv: -0.1683 Rec: 0.0000)
2025-07-27 12:23:17,562 - INFO - Scale [7] Iter [73000/100000] Loss_D: -0.0001 Loss_G: -0.2298 (Adv: -0.2298 Rec: 0.0000)
2025-07-27 12:23:51,551 - INFO - Scale [7] Iter [74000/100000] Loss_D: -0.0001 Loss_G: -0.4984 (Adv: -0.4984 Rec: 0.0000)
2025-07-27 12:24:25,418 - INFO - Scale [7] Iter [75000/100000] Loss_D: 0.0001 Loss_G: -0.3474 (Adv: -0.3474 Rec: 0.0000)
2025-07-27 12:24:59,982 - INFO - Scale [7] Iter [76000/100000] Loss_D: -0.0002 Loss_G: -0.2768 (Adv: -0.2768 Rec: 0.0000)
2025-07-27 12:25:33,766 - INFO - Scale [7] Iter [77000/100000] Loss_D: -0.0002 Loss_G: -0.3064 (Adv: -0.3064 Rec: 0.0000)
2025-07-27 12:26:08,061 - INFO - Scale [7] Iter [78000/100000] Loss_D: -0.0002 Loss_G: -0.4944 (Adv: -0.4944 Rec: 0.0000)
2025-07-27 12:26:42,039 - INFO - Scale [7] Iter [79000/100000] Loss_D: -0.0002 Loss_G: -0.4305 (Adv: -0.4305 Rec: 0.0000)
2025-07-27 12:27:15,215 - INFO - Scale [7] Iter [80000/100000] Loss_D: -0.0002 Loss_G: -0.4928 (Adv: -0.4928 Rec: 0.0000)
2025-07-27 12:27:49,241 - INFO - Scale [7] Iter [81000/100000] Loss_D: -0.0002 Loss_G: -0.5815 (Adv: -0.5815 Rec: 0.0000)
2025-07-27 12:28:22,943 - INFO - Scale [7] Iter [82000/100000] Loss_D: -0.0003 Loss_G: -0.6286 (Adv: -0.6286 Rec: 0.0000)
2025-07-27 12:28:56,839 - INFO - Scale [7] Iter [83000/100000] Loss_D: -0.0001 Loss_G: -0.7260 (Adv: -0.7260 Rec: 0.0000)
2025-07-27 12:29:30,497 - INFO - Scale [7] Iter [84000/100000] Loss_D: -0.0001 Loss_G: -0.6397 (Adv: -0.6397 Rec: 0.0000)
2025-07-27 12:30:04,624 - INFO - Scale [7] Iter [85000/100000] Loss_D: -0.0001 Loss_G: -0.4752 (Adv: -0.4752 Rec: 0.0000)
2025-07-27 12:30:38,485 - INFO - Scale [7] Iter [86000/100000] Loss_D: -0.0002 Loss_G: -0.5091 (Adv: -0.5091 Rec: 0.0000)
2025-07-27 12:31:12,464 - INFO - Scale [7] Iter [87000/100000] Loss_D: -0.0002 Loss_G: -0.4842 (Adv: -0.4842 Rec: 0.0000)
2025-07-27 12:31:46,590 - INFO - Scale [7] Iter [88000/100000] Loss_D: -0.0003 Loss_G: -0.5056 (Adv: -0.5056 Rec: 0.0000)
2025-07-27 12:32:20,953 - INFO - Scale [7] Iter [89000/100000] Loss_D: 0.0000 Loss_G: -0.6250 (Adv: -0.6250 Rec: 0.0000)
2025-07-27 12:32:55,531 - INFO - Scale [7] Iter [90000/100000] Loss_D: -0.0001 Loss_G: -0.6092 (Adv: -0.6092 Rec: 0.0000)
2025-07-27 12:33:29,369 - INFO - Scale [7] Iter [91000/100000] Loss_D: -0.0002 Loss_G: -0.5713 (Adv: -0.5713 Rec: 0.0000)
2025-07-27 12:34:04,258 - INFO - Scale [7] Iter [92000/100000] Loss_D: -0.0002 Loss_G: -0.5546 (Adv: -0.5546 Rec: 0.0000)
2025-07-27 12:34:37,236 - INFO - Scale [7] Iter [93000/100000] Loss_D: -0.0002 Loss_G: -0.5599 (Adv: -0.5599 Rec: 0.0000)
2025-07-27 12:35:12,213 - INFO - Scale [7] Iter [94000/100000] Loss_D: -0.0001 Loss_G: -0.6758 (Adv: -0.6758 Rec: 0.0000)
2025-07-27 12:35:46,380 - INFO - Scale [7] Iter [95000/100000] Loss_D: -0.0001 Loss_G: -0.7300 (Adv: -0.7300 Rec: 0.0000)
2025-07-27 12:36:20,758 - INFO - Scale [7] Iter [96000/100000] Loss_D: -0.0000 Loss_G: -0.6879 (Adv: -0.6879 Rec: 0.0000)
2025-07-27 12:36:55,643 - INFO - Scale [7] Iter [97000/100000] Loss_D: -0.0000 Loss_G: -0.6980 (Adv: -0.6980 Rec: 0.0000)
2025-07-27 12:37:29,890 - INFO - Scale [7] Iter [98000/100000] Loss_D: -0.0002 Loss_G: -0.7632 (Adv: -0.7632 Rec: 0.0000)
2025-07-27 12:38:03,911 - INFO - Scale [7] Iter [99000/100000] Loss_D: -0.0002 Loss_G: -0.8454 (Adv: -0.8454 Rec: 0.0000)
2025-07-27 12:38:38,532 - INFO - 
--- Training Scale 6 ---
2025-07-27 12:38:38,533 - INFO - Volume size: torch.Size([7, 13, 13])
2025-07-27 12:38:38,635 - INFO - Scale [6] Iter [0/100000] Loss_D: 217.7298 Loss_G: 1.3996 (Adv: 0.0670 Rec: 1.3326)
2025-07-27 12:39:13,429 - INFO - Scale [6] Iter [1000/100000] Loss_D: 0.0047 Loss_G: -0.0502 (Adv: -0.0525 Rec: 0.0023)
2025-07-27 12:39:47,593 - INFO - Scale [6] Iter [2000/100000] Loss_D: 0.0067 Loss_G: 0.0418 (Adv: 0.0409 Rec: 0.0009)
2025-07-27 12:40:22,072 - INFO - Scale [6] Iter [3000/100000] Loss_D: 0.0003 Loss_G: -0.1483 (Adv: -0.1493 Rec: 0.0010)
2025-07-27 12:40:57,329 - INFO - Scale [6] Iter [4000/100000] Loss_D: 0.0035 Loss_G: -0.4893 (Adv: -0.4894 Rec: 0.0001)
2025-07-27 12:41:32,322 - INFO - Scale [6] Iter [5000/100000] Loss_D: 0.0003 Loss_G: -0.2729 (Adv: -0.2733 Rec: 0.0004)
2025-07-27 12:42:06,833 - INFO - Scale [6] Iter [6000/100000] Loss_D: 0.0088 Loss_G: -0.2681 (Adv: -0.2682 Rec: 0.0002)
2025-07-27 12:42:40,598 - INFO - Scale [6] Iter [7000/100000] Loss_D: 0.0008 Loss_G: 0.1226 (Adv: 0.1225 Rec: 0.0001)
2025-07-27 12:43:14,014 - INFO - Scale [6] Iter [8000/100000] Loss_D: 0.0054 Loss_G: -0.1660 (Adv: -0.1670 Rec: 0.0010)
2025-07-27 12:43:48,339 - INFO - Scale [6] Iter [9000/100000] Loss_D: 0.0042 Loss_G: 0.1718 (Adv: 0.1715 Rec: 0.0003)
2025-07-27 12:44:22,786 - INFO - Scale [6] Iter [10000/100000] Loss_D: 0.0011 Loss_G: -0.0116 (Adv: -0.0138 Rec: 0.0022)
2025-07-27 12:44:56,904 - INFO - Scale [6] Iter [11000/100000] Loss_D: 0.0005 Loss_G: -0.3227 (Adv: -0.3228 Rec: 0.0001)
2025-07-27 12:45:30,495 - INFO - Scale [6] Iter [12000/100000] Loss_D: 0.0001 Loss_G: -0.3321 (Adv: -0.3327 Rec: 0.0006)
2025-07-27 12:46:04,737 - INFO - Scale [6] Iter [13000/100000] Loss_D: 0.0012 Loss_G: -0.0523 (Adv: -0.0524 Rec: 0.0001)
2025-07-27 12:46:38,553 - INFO - Scale [6] Iter [14000/100000] Loss_D: 0.0027 Loss_G: -0.3044 (Adv: -0.3048 Rec: 0.0004)
2025-07-27 12:47:13,083 - INFO - Scale [6] Iter [15000/100000] Loss_D: 0.0001 Loss_G: -0.6438 (Adv: -0.6442 Rec: 0.0003)
2025-07-27 12:47:47,254 - INFO - Scale [6] Iter [16000/100000] Loss_D: 0.0028 Loss_G: -0.1858 (Adv: -0.1860 Rec: 0.0002)
2025-07-27 12:48:21,630 - INFO - Scale [6] Iter [17000/100000] Loss_D: 0.0104 Loss_G: -0.0295 (Adv: -0.0302 Rec: 0.0007)
2025-07-27 12:48:55,719 - INFO - Scale [6] Iter [18000/100000] Loss_D: 0.0520 Loss_G: -0.4426 (Adv: -0.4428 Rec: 0.0003)
2025-07-27 12:49:31,001 - INFO - Scale [6] Iter [19000/100000] Loss_D: 0.0048 Loss_G: 0.0954 (Adv: 0.0952 Rec: 0.0002)
2025-07-27 12:50:06,771 - INFO - Scale [6] Iter [20000/100000] Loss_D: 0.0032 Loss_G: 0.0241 (Adv: 0.0239 Rec: 0.0002)
2025-07-27 12:50:41,267 - INFO - Scale [6] Iter [21000/100000] Loss_D: 0.0019 Loss_G: 0.4274 (Adv: 0.4272 Rec: 0.0001)
2025-07-27 12:51:15,727 - INFO - Scale [6] Iter [22000/100000] Loss_D: 0.0051 Loss_G: 0.4170 (Adv: 0.4164 Rec: 0.0006)
2025-07-27 12:51:51,214 - INFO - Scale [6] Iter [23000/100000] Loss_D: 0.0015 Loss_G: 0.4514 (Adv: 0.4492 Rec: 0.0022)
2025-07-27 12:52:26,315 - INFO - Scale [6] Iter [24000/100000] Loss_D: 0.0113 Loss_G: 1.1343 (Adv: 1.1341 Rec: 0.0002)
2025-07-27 12:53:01,688 - INFO - Scale [6] Iter [25000/100000] Loss_D: 0.0010 Loss_G: 0.8319 (Adv: 0.8317 Rec: 0.0002)
2025-07-27 12:53:35,156 - INFO - Scale [6] Iter [26000/100000] Loss_D: 0.0035 Loss_G: 0.6701 (Adv: 0.6699 Rec: 0.0002)
2025-07-27 12:54:09,348 - INFO - Scale [6] Iter [27000/100000] Loss_D: 0.0030 Loss_G: 0.4034 (Adv: 0.4029 Rec: 0.0005)
2025-07-27 12:54:43,455 - INFO - Scale [6] Iter [28000/100000] Loss_D: 0.0060 Loss_G: 0.2720 (Adv: 0.2717 Rec: 0.0003)
2025-07-27 12:55:16,981 - INFO - Scale [6] Iter [29000/100000] Loss_D: 0.0027 Loss_G: 0.2742 (Adv: 0.2737 Rec: 0.0004)
2025-07-27 12:55:51,643 - INFO - Scale [6] Iter [30000/100000] Loss_D: 0.0014 Loss_G: -0.1963 (Adv: -0.1964 Rec: 0.0001)
2025-07-27 12:56:24,852 - INFO - Scale [6] Iter [31000/100000] Loss_D: 0.0017 Loss_G: 0.2113 (Adv: 0.2110 Rec: 0.0003)
2025-07-27 12:56:59,731 - INFO - Scale [6] Iter [32000/100000] Loss_D: 0.0004 Loss_G: -0.3402 (Adv: -0.3404 Rec: 0.0003)
2025-07-27 12:57:34,961 - INFO - Scale [6] Iter [33000/100000] Loss_D: 0.0112 Loss_G: -2.6770 (Adv: -2.6774 Rec: 0.0004)
2025-07-27 12:58:09,521 - INFO - Scale [6] Iter [34000/100000] Loss_D: 0.0106 Loss_G: -2.2366 (Adv: -2.2370 Rec: 0.0004)
2025-07-27 12:58:42,314 - INFO - Scale [6] Iter [35000/100000] Loss_D: 0.0068 Loss_G: -2.0083 (Adv: -2.0087 Rec: 0.0005)
2025-07-27 12:59:15,876 - INFO - Scale [6] Iter [36000/100000] Loss_D: 0.0019 Loss_G: -1.7354 (Adv: -1.7359 Rec: 0.0005)
2025-07-27 12:59:50,913 - INFO - Scale [6] Iter [37000/100000] Loss_D: 0.0000 Loss_G: -1.8856 (Adv: -1.8857 Rec: 0.0001)
2025-07-27 13:00:24,725 - INFO - Scale [6] Iter [38000/100000] Loss_D: 0.0002 Loss_G: -1.9194 (Adv: -1.9195 Rec: 0.0001)
2025-07-27 13:00:59,129 - INFO - Scale [6] Iter [39000/100000] Loss_D: 0.0008 Loss_G: -2.1247 (Adv: -2.1248 Rec: 0.0001)
2025-07-27 13:01:32,934 - INFO - Scale [6] Iter [40000/100000] Loss_D: 0.0001 Loss_G: -1.7314 (Adv: -1.7320 Rec: 0.0006)
2025-07-27 13:02:07,230 - INFO - Scale [6] Iter [41000/100000] Loss_D: 0.0014 Loss_G: -1.4178 (Adv: -1.4180 Rec: 0.0002)
2025-07-27 13:02:41,649 - INFO - Scale [6] Iter [42000/100000] Loss_D: 0.0001 Loss_G: -1.8479 (Adv: -1.8481 Rec: 0.0002)
2025-07-27 13:03:15,708 - INFO - Scale [6] Iter [43000/100000] Loss_D: 0.0068 Loss_G: -6.9285 (Adv: -6.9289 Rec: 0.0004)
2025-07-27 13:03:49,413 - INFO - Scale [6] Iter [44000/100000] Loss_D: 0.0037 Loss_G: -6.6320 (Adv: -6.6323 Rec: 0.0003)
2025-07-27 13:04:23,854 - INFO - Scale [6] Iter [45000/100000] Loss_D: 0.0007 Loss_G: -6.4780 (Adv: -6.4784 Rec: 0.0004)
2025-07-27 13:04:56,611 - INFO - Scale [6] Iter [46000/100000] Loss_D: 0.0008 Loss_G: -6.6858 (Adv: -6.6859 Rec: 0.0002)
2025-07-27 13:05:30,319 - INFO - Scale [6] Iter [47000/100000] Loss_D: 0.0001 Loss_G: -6.6607 (Adv: -6.6608 Rec: 0.0001)
2025-07-27 13:06:05,601 - INFO - Scale [6] Iter [48000/100000] Loss_D: 0.0001 Loss_G: -6.3181 (Adv: -6.3184 Rec: 0.0003)
2025-07-27 13:06:39,020 - INFO - Scale [6] Iter [49000/100000] Loss_D: 0.0002 Loss_G: -6.4585 (Adv: -6.4586 Rec: 0.0001)
2025-07-27 13:07:14,245 - INFO - Scale [6] Iter [50000/100000] Loss_D: 0.0011 Loss_G: -6.2658 (Adv: -6.2658 Rec: 0.0001)
2025-07-27 13:07:48,573 - INFO - Scale [6] Iter [51000/100000] Loss_D: 0.0004 Loss_G: -6.1490 (Adv: -6.1490 Rec: 0.0000)
2025-07-27 13:08:22,658 - INFO - Scale [6] Iter [52000/100000] Loss_D: 0.0001 Loss_G: -6.3549 (Adv: -6.3549 Rec: 0.0000)
2025-07-27 13:08:57,169 - INFO - Scale [6] Iter [53000/100000] Loss_D: -0.0001 Loss_G: -6.5768 (Adv: -6.5768 Rec: 0.0000)
2025-07-27 13:09:27,222 - INFO - Scale [6] Iter [54000/100000] Loss_D: -0.0001 Loss_G: -6.3672 (Adv: -6.3673 Rec: 0.0000)
2025-07-27 13:09:58,413 - INFO - Scale [6] Iter [55000/100000] Loss_D: -0.0003 Loss_G: -6.0701 (Adv: -6.0701 Rec: 0.0000)
2025-07-27 13:10:31,830 - INFO - Scale [6] Iter [56000/100000] Loss_D: 0.0002 Loss_G: -5.8951 (Adv: -5.8951 Rec: 0.0000)
2025-07-27 13:11:05,600 - INFO - Scale [6] Iter [57000/100000] Loss_D: 0.0001 Loss_G: -5.9090 (Adv: -5.9090 Rec: 0.0000)
2025-07-27 13:11:39,519 - INFO - Scale [6] Iter [58000/100000] Loss_D: -0.0003 Loss_G: -5.9291 (Adv: -5.9291 Rec: 0.0000)
2025-07-27 13:12:13,931 - INFO - Scale [6] Iter [59000/100000] Loss_D: -0.0002 Loss_G: -6.0249 (Adv: -6.0249 Rec: 0.0000)
2025-07-27 13:12:48,158 - INFO - Scale [6] Iter [60000/100000] Loss_D: -0.0001 Loss_G: -5.9549 (Adv: -5.9549 Rec: 0.0000)
2025-07-27 13:13:22,114 - INFO - Scale [6] Iter [61000/100000] Loss_D: 0.0001 Loss_G: -5.8403 (Adv: -5.8403 Rec: 0.0000)
2025-07-27 13:13:56,411 - INFO - Scale [6] Iter [62000/100000] Loss_D: 0.0002 Loss_G: -5.7227 (Adv: -5.7227 Rec: 0.0000)
2025-07-27 13:14:31,005 - INFO - Scale [6] Iter [63000/100000] Loss_D: -0.0002 Loss_G: -5.7086 (Adv: -5.7086 Rec: 0.0000)
2025-07-27 13:15:05,278 - INFO - Scale [6] Iter [64000/100000] Loss_D: 0.0006 Loss_G: -5.6758 (Adv: -5.6758 Rec: 0.0000)
2025-07-27 13:15:38,853 - INFO - Scale [6] Iter [65000/100000] Loss_D: -0.0001 Loss_G: -5.9087 (Adv: -5.9088 Rec: 0.0000)
2025-07-27 13:16:13,016 - INFO - Scale [6] Iter [66000/100000] Loss_D: 0.0005 Loss_G: -5.7773 (Adv: -5.7773 Rec: 0.0000)
2025-07-27 13:16:47,122 - INFO - Scale [6] Iter [67000/100000] Loss_D: -0.0004 Loss_G: -5.8244 (Adv: -5.8244 Rec: 0.0000)
2025-07-27 13:17:20,255 - INFO - Scale [6] Iter [68000/100000] Loss_D: 0.0003 Loss_G: -5.8496 (Adv: -5.8496 Rec: 0.0000)
2025-07-27 13:17:55,293 - INFO - Scale [6] Iter [69000/100000] Loss_D: -0.0001 Loss_G: -6.0367 (Adv: -6.0367 Rec: 0.0000)
2025-07-27 13:18:29,798 - INFO - Scale [6] Iter [70000/100000] Loss_D: -0.0001 Loss_G: -5.7433 (Adv: -5.7433 Rec: 0.0000)
2025-07-27 13:19:04,781 - INFO - Scale [6] Iter [71000/100000] Loss_D: 0.0002 Loss_G: -5.8127 (Adv: -5.8127 Rec: 0.0000)
2025-07-27 13:19:38,687 - INFO - Scale [6] Iter [72000/100000] Loss_D: 0.0000 Loss_G: -5.8440 (Adv: -5.8440 Rec: 0.0000)
2025-07-27 13:20:13,392 - INFO - Scale [6] Iter [73000/100000] Loss_D: 0.0001 Loss_G: -5.7663 (Adv: -5.7663 Rec: 0.0000)
2025-07-27 13:20:47,530 - INFO - Scale [6] Iter [74000/100000] Loss_D: -0.0001 Loss_G: -5.7149 (Adv: -5.7149 Rec: 0.0000)
2025-07-27 13:21:22,548 - INFO - Scale [6] Iter [75000/100000] Loss_D: -0.0001 Loss_G: -5.7665 (Adv: -5.7665 Rec: 0.0000)
2025-07-27 13:21:58,323 - INFO - Scale [6] Iter [76000/100000] Loss_D: -0.0003 Loss_G: -5.7276 (Adv: -5.7276 Rec: 0.0000)
2025-07-27 13:22:32,920 - INFO - Scale [6] Iter [77000/100000] Loss_D: -0.0005 Loss_G: -5.7313 (Adv: -5.7313 Rec: 0.0000)
2025-07-27 13:23:07,354 - INFO - Scale [6] Iter [78000/100000] Loss_D: -0.0003 Loss_G: -5.7223 (Adv: -5.7223 Rec: 0.0000)
2025-07-27 13:23:41,194 - INFO - Scale [6] Iter [79000/100000] Loss_D: -0.0003 Loss_G: -5.7411 (Adv: -5.7411 Rec: 0.0000)
2025-07-27 13:24:15,936 - INFO - Scale [6] Iter [80000/100000] Loss_D: -0.0003 Loss_G: -5.7379 (Adv: -5.7379 Rec: 0.0000)
2025-07-27 13:24:51,007 - INFO - Scale [6] Iter [81000/100000] Loss_D: 0.0001 Loss_G: -5.8208 (Adv: -5.8208 Rec: 0.0000)
2025-07-27 13:25:24,831 - INFO - Scale [6] Iter [82000/100000] Loss_D: -0.0001 Loss_G: -5.7981 (Adv: -5.7981 Rec: 0.0000)
2025-07-27 13:25:59,380 - INFO - Scale [6] Iter [83000/100000] Loss_D: 0.0000 Loss_G: -5.8106 (Adv: -5.8106 Rec: 0.0000)
2025-07-27 13:26:34,274 - INFO - Scale [6] Iter [84000/100000] Loss_D: -0.0004 Loss_G: -5.8257 (Adv: -5.8257 Rec: 0.0000)
2025-07-27 13:27:08,891 - INFO - Scale [6] Iter [85000/100000] Loss_D: -0.0005 Loss_G: -5.8255 (Adv: -5.8255 Rec: 0.0000)
2025-07-27 13:27:43,064 - INFO - Scale [6] Iter [86000/100000] Loss_D: -0.0004 Loss_G: -5.8163 (Adv: -5.8163 Rec: 0.0000)
2025-07-27 13:28:17,559 - INFO - Scale [6] Iter [87000/100000] Loss_D: -0.0004 Loss_G: -5.7591 (Adv: -5.7591 Rec: 0.0000)
2025-07-27 13:28:51,530 - INFO - Scale [6] Iter [88000/100000] Loss_D: -0.0003 Loss_G: -5.7661 (Adv: -5.7661 Rec: 0.0000)
2025-07-27 13:29:26,401 - INFO - Scale [6] Iter [89000/100000] Loss_D: 0.0004 Loss_G: -5.7596 (Adv: -5.7596 Rec: 0.0000)
2025-07-27 13:30:01,672 - INFO - Scale [6] Iter [90000/100000] Loss_D: -0.0000 Loss_G: -5.7520 (Adv: -5.7520 Rec: 0.0000)
2025-07-27 13:30:36,524 - INFO - Scale [6] Iter [91000/100000] Loss_D: -0.0005 Loss_G: -5.7668 (Adv: -5.7668 Rec: 0.0000)
2025-07-27 13:31:11,654 - INFO - Scale [6] Iter [92000/100000] Loss_D: 0.0000 Loss_G: -5.7438 (Adv: -5.7438 Rec: 0.0000)
2025-07-27 13:31:46,290 - INFO - Scale [6] Iter [93000/100000] Loss_D: -0.0003 Loss_G: -5.7169 (Adv: -5.7169 Rec: 0.0000)
2025-07-27 13:32:20,913 - INFO - Scale [6] Iter [94000/100000] Loss_D: -0.0003 Loss_G: -5.7356 (Adv: -5.7356 Rec: 0.0000)
2025-07-27 13:32:55,609 - INFO - Scale [6] Iter [95000/100000] Loss_D: -0.0003 Loss_G: -5.7404 (Adv: -5.7404 Rec: 0.0000)
2025-07-27 13:33:30,178 - INFO - Scale [6] Iter [96000/100000] Loss_D: -0.0004 Loss_G: -5.7429 (Adv: -5.7429 Rec: 0.0000)
2025-07-27 13:34:05,005 - INFO - Scale [6] Iter [97000/100000] Loss_D: -0.0002 Loss_G: -5.7475 (Adv: -5.7475 Rec: 0.0000)
2025-07-27 13:34:39,414 - INFO - Scale [6] Iter [98000/100000] Loss_D: -0.0003 Loss_G: -5.7370 (Adv: -5.7370 Rec: 0.0000)
2025-07-27 13:35:13,964 - INFO - Scale [6] Iter [99000/100000] Loss_D: -0.0002 Loss_G: -5.7105 (Adv: -5.7105 Rec: 0.0000)
2025-07-27 13:35:48,994 - INFO - 
--- Training Scale 5 ---
2025-07-27 13:35:48,995 - INFO - Volume size: torch.Size([8, 15, 15])
2025-07-27 13:35:49,099 - INFO - Scale [5] Iter [0/100000] Loss_D: 285.3914 Loss_G: 1.7393 (Adv: 0.2169 Rec: 1.5224)
2025-07-27 13:36:22,925 - INFO - Scale [5] Iter [1000/100000] Loss_D: -0.0001 Loss_G: 0.0354 (Adv: 0.0239 Rec: 0.0115)
2025-07-27 13:36:58,542 - INFO - Scale [5] Iter [2000/100000] Loss_D: 0.0232 Loss_G: 0.1960 (Adv: 0.1951 Rec: 0.0009)
2025-07-27 13:37:32,666 - INFO - Scale [5] Iter [3000/100000] Loss_D: 1.0235 Loss_G: 0.2661 (Adv: 0.2624 Rec: 0.0037)
2025-07-27 13:38:07,789 - INFO - Scale [5] Iter [4000/100000] Loss_D: 0.0009 Loss_G: 0.0793 (Adv: 0.0761 Rec: 0.0033)
2025-07-27 13:38:42,704 - INFO - Scale [5] Iter [5000/100000] Loss_D: -0.0002 Loss_G: -0.0217 (Adv: -0.0221 Rec: 0.0003)
2025-07-27 13:39:17,122 - INFO - Scale [5] Iter [6000/100000] Loss_D: -0.0005 Loss_G: -0.0935 (Adv: -0.0946 Rec: 0.0011)
2025-07-27 13:39:52,198 - INFO - Scale [5] Iter [7000/100000] Loss_D: 0.0020 Loss_G: 0.2319 (Adv: 0.2311 Rec: 0.0008)
2025-07-27 13:40:27,239 - INFO - Scale [5] Iter [8000/100000] Loss_D: 0.0012 Loss_G: 0.3458 (Adv: 0.3449 Rec: 0.0009)
2025-07-27 13:41:02,208 - INFO - Scale [5] Iter [9000/100000] Loss_D: 0.0001 Loss_G: -0.0906 (Adv: -0.0917 Rec: 0.0011)
2025-07-27 13:41:37,236 - INFO - Scale [5] Iter [10000/100000] Loss_D: 0.0002 Loss_G: -0.0060 (Adv: -0.0067 Rec: 0.0007)
2025-07-27 13:42:11,966 - INFO - Scale [5] Iter [11000/100000] Loss_D: 0.0069 Loss_G: -0.4746 (Adv: -0.4756 Rec: 0.0011)
2025-07-27 13:42:45,470 - INFO - Scale [5] Iter [12000/100000] Loss_D: 0.0002 Loss_G: -0.2164 (Adv: -0.2168 Rec: 0.0004)
2025-07-27 13:43:19,544 - INFO - Scale [5] Iter [13000/100000] Loss_D: 0.0029 Loss_G: -0.5593 (Adv: -0.5603 Rec: 0.0010)
2025-07-27 13:43:53,898 - INFO - Scale [5] Iter [14000/100000] Loss_D: 0.0054 Loss_G: -0.0698 (Adv: -0.0700 Rec: 0.0002)
2025-07-27 13:44:28,608 - INFO - Scale [5] Iter [15000/100000] Loss_D: -0.0007 Loss_G: -0.7721 (Adv: -0.7745 Rec: 0.0025)
2025-07-27 13:45:04,171 - INFO - Scale [5] Iter [16000/100000] Loss_D: 0.0007 Loss_G: -0.5397 (Adv: -0.5401 Rec: 0.0004)
2025-07-27 13:45:39,079 - INFO - Scale [5] Iter [17000/100000] Loss_D: 0.0032 Loss_G: -0.4887 (Adv: -0.4891 Rec: 0.0004)
2025-07-27 13:46:13,435 - INFO - Scale [5] Iter [18000/100000] Loss_D: 0.0014 Loss_G: 0.0662 (Adv: 0.0661 Rec: 0.0002)
2025-07-27 13:46:47,638 - INFO - Scale [5] Iter [19000/100000] Loss_D: 0.0035 Loss_G: -0.4391 (Adv: -0.4405 Rec: 0.0014)
2025-07-27 13:47:22,188 - INFO - Scale [5] Iter [20000/100000] Loss_D: 0.0020 Loss_G: -0.4543 (Adv: -0.4545 Rec: 0.0002)
2025-07-27 13:47:57,147 - INFO - Scale [5] Iter [21000/100000] Loss_D: 0.0002 Loss_G: -0.7803 (Adv: -0.7810 Rec: 0.0007)
2025-07-27 13:48:31,602 - INFO - Scale [5] Iter [22000/100000] Loss_D: 0.0009 Loss_G: -0.8795 (Adv: -0.8797 Rec: 0.0002)
2025-07-27 13:49:06,121 - INFO - Scale [5] Iter [23000/100000] Loss_D: 0.0016 Loss_G: -3.7783 (Adv: -3.7786 Rec: 0.0002)
2025-07-27 13:49:41,157 - INFO - Scale [5] Iter [24000/100000] Loss_D: 0.0027 Loss_G: -3.3219 (Adv: -3.3237 Rec: 0.0018)
2025-07-27 13:50:15,688 - INFO - Scale [5] Iter [25000/100000] Loss_D: 0.0050 Loss_G: -3.1118 (Adv: -3.1120 Rec: 0.0003)
2025-07-27 13:50:50,378 - INFO - Scale [5] Iter [26000/100000] Loss_D: 0.0014 Loss_G: -3.2542 (Adv: -3.2544 Rec: 0.0001)
2025-07-27 13:51:24,976 - INFO - Scale [5] Iter [27000/100000] Loss_D: 0.0004 Loss_G: -3.3297 (Adv: -3.3299 Rec: 0.0002)
2025-07-27 13:52:00,195 - INFO - Scale [5] Iter [28000/100000] Loss_D: 0.0013 Loss_G: -3.4450 (Adv: -3.4453 Rec: 0.0003)
2025-07-27 13:52:34,883 - INFO - Scale [5] Iter [29000/100000] Loss_D: 0.0002 Loss_G: -3.0808 (Adv: -3.0812 Rec: 0.0004)
2025-07-27 13:53:10,016 - INFO - Scale [5] Iter [30000/100000] Loss_D: 0.0003 Loss_G: -2.4683 (Adv: -2.4684 Rec: 0.0001)
2025-07-27 13:53:44,520 - INFO - Scale [5] Iter [31000/100000] Loss_D: 0.0002 Loss_G: -2.2885 (Adv: -2.2888 Rec: 0.0002)
2025-07-27 13:54:18,070 - INFO - Scale [5] Iter [32000/100000] Loss_D: 0.0005 Loss_G: -1.7104 (Adv: -1.7108 Rec: 0.0004)
2025-07-27 13:54:53,501 - INFO - Scale [5] Iter [33000/100000] Loss_D: 0.0006 Loss_G: -0.9324 (Adv: -0.9331 Rec: 0.0007)
2025-07-27 13:55:27,390 - INFO - Scale [5] Iter [34000/100000] Loss_D: 0.0007 Loss_G: -0.4679 (Adv: -0.4681 Rec: 0.0002)
2025-07-27 13:56:03,075 - INFO - Scale [5] Iter [35000/100000] Loss_D: 0.0042 Loss_G: -1.8653 (Adv: -1.8654 Rec: 0.0001)
2025-07-27 13:56:37,426 - INFO - Scale [5] Iter [36000/100000] Loss_D: 0.0033 Loss_G: -1.1620 (Adv: -1.1621 Rec: 0.0001)
2025-07-27 13:57:12,581 - INFO - Scale [5] Iter [37000/100000] Loss_D: 0.0009 Loss_G: -0.2461 (Adv: -0.2463 Rec: 0.0002)
2025-07-27 13:57:46,738 - INFO - Scale [5] Iter [38000/100000] Loss_D: 0.0036 Loss_G: 0.2225 (Adv: 0.2224 Rec: 0.0001)
2025-07-27 13:58:20,407 - INFO - Scale [5] Iter [39000/100000] Loss_D: 0.0000 Loss_G: -0.0500 (Adv: -0.0503 Rec: 0.0002)
2025-07-27 13:58:54,607 - INFO - Scale [5] Iter [40000/100000] Loss_D: 0.0002 Loss_G: -0.4122 (Adv: -0.4124 Rec: 0.0002)
2025-07-27 13:59:30,097 - INFO - Scale [5] Iter [41000/100000] Loss_D: 0.0002 Loss_G: -0.3599 (Adv: -0.3602 Rec: 0.0003)
2025-07-27 14:00:05,298 - INFO - Scale [5] Iter [42000/100000] Loss_D: 0.0007 Loss_G: -0.2697 (Adv: -0.2701 Rec: 0.0003)
2025-07-27 14:00:39,491 - INFO - Scale [5] Iter [43000/100000] Loss_D: 0.0000 Loss_G: -0.8818 (Adv: -0.8820 Rec: 0.0002)
2025-07-27 14:01:14,085 - INFO - Scale [5] Iter [44000/100000] Loss_D: -0.0001 Loss_G: -0.5243 (Adv: -0.5244 Rec: 0.0001)
2025-07-27 14:01:49,725 - INFO - Scale [5] Iter [45000/100000] Loss_D: 0.0045 Loss_G: -1.4324 (Adv: -1.4326 Rec: 0.0002)
2025-07-27 14:02:24,402 - INFO - Scale [5] Iter [46000/100000] Loss_D: 0.0021 Loss_G: -0.8629 (Adv: -0.8632 Rec: 0.0003)
2025-07-27 14:02:58,734 - INFO - Scale [5] Iter [47000/100000] Loss_D: 0.0014 Loss_G: -0.8099 (Adv: -0.8102 Rec: 0.0003)
2025-07-27 14:03:32,792 - INFO - Scale [5] Iter [48000/100000] Loss_D: 0.0001 Loss_G: -0.7128 (Adv: -0.7128 Rec: 0.0001)
2025-07-27 14:04:07,832 - INFO - Scale [5] Iter [49000/100000] Loss_D: 0.0002 Loss_G: -0.5845 (Adv: -0.5849 Rec: 0.0004)
2025-07-27 14:04:43,113 - INFO - Scale [5] Iter [50000/100000] Loss_D: 0.0002 Loss_G: -1.2168 (Adv: -1.2169 Rec: 0.0001)
2025-07-27 14:05:17,508 - INFO - Scale [5] Iter [51000/100000] Loss_D: -0.0001 Loss_G: -1.1707 (Adv: -1.1707 Rec: 0.0000)
2025-07-27 14:05:52,290 - INFO - Scale [5] Iter [52000/100000] Loss_D: -0.0001 Loss_G: -1.3003 (Adv: -1.3003 Rec: 0.0000)
2025-07-27 14:06:26,519 - INFO - Scale [5] Iter [53000/100000] Loss_D: -0.0002 Loss_G: -1.3306 (Adv: -1.3306 Rec: 0.0000)
2025-07-27 14:07:01,486 - INFO - Scale [5] Iter [54000/100000] Loss_D: -0.0003 Loss_G: -1.4214 (Adv: -1.4214 Rec: 0.0000)
2025-07-27 14:07:36,229 - INFO - Scale [5] Iter [55000/100000] Loss_D: 0.0001 Loss_G: -1.1372 (Adv: -1.1372 Rec: 0.0000)
2025-07-27 14:08:11,268 - INFO - Scale [5] Iter [56000/100000] Loss_D: -0.0002 Loss_G: -1.1761 (Adv: -1.1761 Rec: 0.0000)
2025-07-27 14:08:46,101 - INFO - Scale [5] Iter [57000/100000] Loss_D: -0.0001 Loss_G: -1.6607 (Adv: -1.6607 Rec: 0.0000)
2025-07-27 14:09:20,801 - INFO - Scale [5] Iter [58000/100000] Loss_D: -0.0003 Loss_G: -1.7257 (Adv: -1.7257 Rec: 0.0000)
2025-07-27 14:09:55,426 - INFO - Scale [5] Iter [59000/100000] Loss_D: -0.0003 Loss_G: -1.7150 (Adv: -1.7150 Rec: 0.0000)
2025-07-27 14:10:29,916 - INFO - Scale [5] Iter [60000/100000] Loss_D: -0.0004 Loss_G: -1.8381 (Adv: -1.8381 Rec: 0.0000)
2025-07-27 14:11:04,493 - INFO - Scale [5] Iter [61000/100000] Loss_D: -0.0004 Loss_G: -1.6130 (Adv: -1.6130 Rec: 0.0000)
2025-07-27 14:11:39,437 - INFO - Scale [5] Iter [62000/100000] Loss_D: -0.0002 Loss_G: -1.5837 (Adv: -1.5837 Rec: 0.0000)
2025-07-27 14:12:13,711 - INFO - Scale [5] Iter [63000/100000] Loss_D: -0.0004 Loss_G: -1.4031 (Adv: -1.4031 Rec: 0.0000)
2025-07-27 14:12:48,055 - INFO - Scale [5] Iter [64000/100000] Loss_D: -0.0006 Loss_G: -1.5178 (Adv: -1.5178 Rec: 0.0000)
2025-07-27 14:13:22,953 - INFO - Scale [5] Iter [65000/100000] Loss_D: -0.0004 Loss_G: -1.1538 (Adv: -1.1538 Rec: 0.0000)
2025-07-27 14:13:57,618 - INFO - Scale [5] Iter [66000/100000] Loss_D: -0.0004 Loss_G: -1.2407 (Adv: -1.2407 Rec: 0.0000)
2025-07-27 14:14:32,039 - INFO - Scale [5] Iter [67000/100000] Loss_D: -0.0005 Loss_G: -1.0814 (Adv: -1.0814 Rec: 0.0000)
2025-07-27 14:15:06,122 - INFO - Scale [5] Iter [68000/100000] Loss_D: -0.0003 Loss_G: -1.1681 (Adv: -1.1681 Rec: 0.0000)
2025-07-27 14:15:40,623 - INFO - Scale [5] Iter [69000/100000] Loss_D: -0.0005 Loss_G: -0.9934 (Adv: -0.9934 Rec: 0.0000)
2025-07-27 14:16:15,600 - INFO - Scale [5] Iter [70000/100000] Loss_D: -0.0005 Loss_G: -0.9859 (Adv: -0.9859 Rec: 0.0000)
2025-07-27 14:16:50,223 - INFO - Scale [5] Iter [71000/100000] Loss_D: -0.0002 Loss_G: -1.0482 (Adv: -1.0482 Rec: 0.0000)
2025-07-27 14:17:23,836 - INFO - Scale [5] Iter [72000/100000] Loss_D: -0.0004 Loss_G: -0.9822 (Adv: -0.9822 Rec: 0.0000)
2025-07-27 14:17:58,299 - INFO - Scale [5] Iter [73000/100000] Loss_D: -0.0006 Loss_G: -1.0138 (Adv: -1.0138 Rec: 0.0000)
2025-07-27 14:18:32,964 - INFO - Scale [5] Iter [74000/100000] Loss_D: -0.0006 Loss_G: -0.9261 (Adv: -0.9261 Rec: 0.0000)
2025-07-27 14:19:07,707 - INFO - Scale [5] Iter [75000/100000] Loss_D: -0.0003 Loss_G: -0.8486 (Adv: -0.8487 Rec: 0.0000)
2025-07-27 14:19:42,195 - INFO - Scale [5] Iter [76000/100000] Loss_D: -0.0005 Loss_G: -0.8853 (Adv: -0.8853 Rec: 0.0000)
2025-07-27 14:20:17,160 - INFO - Scale [5] Iter [77000/100000] Loss_D: -0.0004 Loss_G: -0.7738 (Adv: -0.7738 Rec: 0.0000)
2025-07-27 14:20:51,859 - INFO - Scale [5] Iter [78000/100000] Loss_D: -0.0005 Loss_G: -0.7796 (Adv: -0.7796 Rec: 0.0000)
2025-07-27 14:21:26,766 - INFO - Scale [5] Iter [79000/100000] Loss_D: -0.0007 Loss_G: -0.7940 (Adv: -0.7940 Rec: 0.0000)
2025-07-27 14:22:02,108 - INFO - Scale [5] Iter [80000/100000] Loss_D: -0.0006 Loss_G: -0.7782 (Adv: -0.7782 Rec: 0.0000)
2025-07-27 14:22:36,146 - INFO - Scale [5] Iter [81000/100000] Loss_D: -0.0003 Loss_G: -0.6938 (Adv: -0.6938 Rec: 0.0000)
2025-07-27 14:23:10,743 - INFO - Scale [5] Iter [82000/100000] Loss_D: -0.0005 Loss_G: -0.7223 (Adv: -0.7223 Rec: 0.0000)
2025-07-27 14:23:44,345 - INFO - Scale [5] Iter [83000/100000] Loss_D: -0.0004 Loss_G: -0.6601 (Adv: -0.6601 Rec: 0.0000)
2025-07-27 14:24:18,945 - INFO - Scale [5] Iter [84000/100000] Loss_D: -0.0001 Loss_G: -0.6777 (Adv: -0.6777 Rec: 0.0000)
2025-07-27 14:24:54,099 - INFO - Scale [5] Iter [85000/100000] Loss_D: -0.0002 Loss_G: -0.7566 (Adv: -0.7566 Rec: 0.0000)
2025-07-27 14:25:29,112 - INFO - Scale [5] Iter [86000/100000] Loss_D: -0.0005 Loss_G: -0.6940 (Adv: -0.6940 Rec: 0.0000)
2025-07-27 14:26:04,357 - INFO - Scale [5] Iter [87000/100000] Loss_D: -0.0004 Loss_G: -0.7035 (Adv: -0.7035 Rec: 0.0000)
2025-07-27 14:26:38,532 - INFO - Scale [5] Iter [88000/100000] Loss_D: -0.0002 Loss_G: -0.7108 (Adv: -0.7108 Rec: 0.0000)
2025-07-27 14:27:12,673 - INFO - Scale [5] Iter [89000/100000] Loss_D: -0.0005 Loss_G: -0.6926 (Adv: -0.6926 Rec: 0.0000)
2025-07-27 14:27:47,453 - INFO - Scale [5] Iter [90000/100000] Loss_D: -0.0004 Loss_G: -0.7333 (Adv: -0.7333 Rec: 0.0000)
2025-07-27 14:28:22,331 - INFO - Scale [5] Iter [91000/100000] Loss_D: -0.0007 Loss_G: -0.7259 (Adv: -0.7259 Rec: 0.0000)
2025-07-27 14:28:56,959 - INFO - Scale [5] Iter [92000/100000] Loss_D: -0.0004 Loss_G: -0.7890 (Adv: -0.7890 Rec: 0.0000)
2025-07-27 14:29:30,955 - INFO - Scale [5] Iter [93000/100000] Loss_D: -0.0005 Loss_G: -0.8023 (Adv: -0.8023 Rec: 0.0000)
2025-07-27 14:30:05,680 - INFO - Scale [5] Iter [94000/100000] Loss_D: -0.0006 Loss_G: -0.8538 (Adv: -0.8538 Rec: 0.0000)
2025-07-27 14:30:40,374 - INFO - Scale [5] Iter [95000/100000] Loss_D: -0.0004 Loss_G: -0.8031 (Adv: -0.8031 Rec: 0.0000)
2025-07-27 14:31:14,651 - INFO - Scale [5] Iter [96000/100000] Loss_D: -0.0005 Loss_G: -0.8913 (Adv: -0.8913 Rec: 0.0000)
2025-07-27 14:31:49,113 - INFO - Scale [5] Iter [97000/100000] Loss_D: -0.0005 Loss_G: -0.9153 (Adv: -0.9153 Rec: 0.0000)
2025-07-27 14:32:23,340 - INFO - Scale [5] Iter [98000/100000] Loss_D: -0.0005 Loss_G: -0.9591 (Adv: -0.9591 Rec: 0.0000)
2025-07-27 14:32:57,953 - INFO - Scale [5] Iter [99000/100000] Loss_D: -0.0006 Loss_G: -0.9756 (Adv: -0.9756 Rec: 0.0000)
2025-07-27 14:33:32,827 - INFO - 
--- Training Scale 4 ---
2025-07-27 14:33:32,828 - INFO - Volume size: torch.Size([9, 17, 17])
2025-07-27 14:33:32,932 - INFO - Scale [4] Iter [0/100000] Loss_D: 305.5734 Loss_G: 1.3136 (Adv: -0.1052 Rec: 1.4189)
2025-07-27 14:34:11,907 - INFO - Scale [4] Iter [1000/100000] Loss_D: 0.1031 Loss_G: 0.0067 (Adv: 0.0059 Rec: 0.0007)
2025-07-27 14:34:51,014 - INFO - Scale [4] Iter [2000/100000] Loss_D: 0.0009 Loss_G: -0.0241 (Adv: -0.0272 Rec: 0.0030)
2025-07-27 14:35:30,056 - INFO - Scale [4] Iter [3000/100000] Loss_D: 0.0045 Loss_G: 0.0922 (Adv: 0.0887 Rec: 0.0034)
2025-07-27 14:36:09,230 - INFO - Scale [4] Iter [4000/100000] Loss_D: -0.0002 Loss_G: 0.3887 (Adv: 0.3871 Rec: 0.0016)
2025-07-27 14:36:48,596 - INFO - Scale [4] Iter [5000/100000] Loss_D: -0.0001 Loss_G: 0.2106 (Adv: 0.2073 Rec: 0.0033)
2025-07-27 14:37:27,560 - INFO - Scale [4] Iter [6000/100000] Loss_D: -0.0001 Loss_G: 0.1573 (Adv: 0.1571 Rec: 0.0002)
2025-07-27 14:38:06,602 - INFO - Scale [4] Iter [7000/100000] Loss_D: 0.0178 Loss_G: -0.4867 (Adv: -0.4884 Rec: 0.0018)
2025-07-27 14:38:45,903 - INFO - Scale [4] Iter [8000/100000] Loss_D: 0.0023 Loss_G: -0.4177 (Adv: -0.4195 Rec: 0.0018)
2025-07-27 14:39:24,806 - INFO - Scale [4] Iter [9000/100000] Loss_D: 0.0234 Loss_G: -0.1370 (Adv: -0.1400 Rec: 0.0030)
2025-07-27 14:40:04,188 - INFO - Scale [4] Iter [10000/100000] Loss_D: 0.0016 Loss_G: -0.1699 (Adv: -0.1702 Rec: 0.0002)
2025-07-27 14:40:43,255 - INFO - Scale [4] Iter [11000/100000] Loss_D: -0.0004 Loss_G: -0.1937 (Adv: -0.1947 Rec: 0.0010)
2025-07-27 14:41:22,438 - INFO - Scale [4] Iter [12000/100000] Loss_D: -0.0001 Loss_G: -0.1598 (Adv: -0.1602 Rec: 0.0004)
2025-07-27 14:42:01,419 - INFO - Scale [4] Iter [13000/100000] Loss_D: 0.0011 Loss_G: -0.0911 (Adv: -0.0936 Rec: 0.0024)
2025-07-27 14:42:40,587 - INFO - Scale [4] Iter [14000/100000] Loss_D: -0.0002 Loss_G: -0.1350 (Adv: -0.1353 Rec: 0.0003)
2025-07-27 14:43:19,526 - INFO - Scale [4] Iter [15000/100000] Loss_D: 0.0005 Loss_G: -0.1679 (Adv: -0.1685 Rec: 0.0006)
2025-07-27 14:43:58,356 - INFO - Scale [4] Iter [16000/100000] Loss_D: 0.0019 Loss_G: -0.0811 (Adv: -0.0813 Rec: 0.0002)
2025-07-27 14:44:37,760 - INFO - Scale [4] Iter [17000/100000] Loss_D: -0.0003 Loss_G: -0.0062 (Adv: -0.0065 Rec: 0.0003)
2025-07-27 14:45:16,710 - INFO - Scale [4] Iter [18000/100000] Loss_D: 0.0007 Loss_G: -0.0082 (Adv: -0.0084 Rec: 0.0002)
2025-07-27 14:45:55,714 - INFO - Scale [4] Iter [19000/100000] Loss_D: 0.0005 Loss_G: -0.0886 (Adv: -0.0889 Rec: 0.0002)
2025-07-27 14:46:34,924 - INFO - Scale [4] Iter [20000/100000] Loss_D: -0.0001 Loss_G: 0.0973 (Adv: 0.0970 Rec: 0.0003)
2025-07-27 14:47:14,278 - INFO - Scale [4] Iter [21000/100000] Loss_D: 0.0013 Loss_G: 0.1518 (Adv: 0.1514 Rec: 0.0005)
2025-07-27 14:47:53,408 - INFO - Scale [4] Iter [22000/100000] Loss_D: 0.0017 Loss_G: 0.1882 (Adv: 0.1879 Rec: 0.0003)
2025-07-27 14:48:32,718 - INFO - Scale [4] Iter [23000/100000] Loss_D: 0.0001 Loss_G: 0.0757 (Adv: 0.0742 Rec: 0.0015)
2025-07-27 14:49:11,742 - INFO - Scale [4] Iter [24000/100000] Loss_D: 0.0005 Loss_G: 0.2144 (Adv: 0.2143 Rec: 0.0001)
2025-07-27 14:49:50,854 - INFO - Scale [4] Iter [25000/100000] Loss_D: 0.0001 Loss_G: 0.0947 (Adv: 0.0946 Rec: 0.0002)
2025-07-27 14:50:29,813 - INFO - Scale [4] Iter [26000/100000] Loss_D: 0.0028 Loss_G: 0.2014 (Adv: 0.2008 Rec: 0.0006)
2025-07-27 14:51:08,713 - INFO - Scale [4] Iter [27000/100000] Loss_D: 0.0002 Loss_G: 0.3628 (Adv: 0.3626 Rec: 0.0002)
2025-07-27 14:51:47,725 - INFO - Scale [4] Iter [28000/100000] Loss_D: 0.0005 Loss_G: 0.3279 (Adv: 0.3272 Rec: 0.0007)
2025-07-27 14:52:26,892 - INFO - Scale [4] Iter [29000/100000] Loss_D: 0.0032 Loss_G: 0.4048 (Adv: 0.4045 Rec: 0.0003)
2025-07-27 14:53:05,842 - INFO - Scale [4] Iter [30000/100000] Loss_D: 0.0049 Loss_G: 0.2687 (Adv: 0.2683 Rec: 0.0004)
2025-07-27 14:53:44,959 - INFO - Scale [4] Iter [31000/100000] Loss_D: 0.0008 Loss_G: 0.3034 (Adv: 0.3029 Rec: 0.0005)
2025-07-27 14:54:23,932 - INFO - Scale [4] Iter [32000/100000] Loss_D: 0.0003 Loss_G: 0.3130 (Adv: 0.3126 Rec: 0.0005)
2025-07-27 14:55:03,223 - INFO - Scale [4] Iter [33000/100000] Loss_D: -0.0002 Loss_G: 0.4040 (Adv: 0.4034 Rec: 0.0006)
2025-07-27 14:55:42,165 - INFO - Scale [4] Iter [34000/100000] Loss_D: 0.0008 Loss_G: 0.2274 (Adv: 0.2273 Rec: 0.0001)
2025-07-27 14:56:21,376 - INFO - Scale [4] Iter [35000/100000] Loss_D: -0.0003 Loss_G: 0.4371 (Adv: 0.4370 Rec: 0.0001)
2025-07-27 14:57:00,391 - INFO - Scale [4] Iter [36000/100000] Loss_D: -0.0002 Loss_G: 0.4147 (Adv: 0.4143 Rec: 0.0003)
2025-07-27 14:57:39,539 - INFO - Scale [4] Iter [37000/100000] Loss_D: 0.0001 Loss_G: 0.1675 (Adv: 0.1673 Rec: 0.0002)
2025-07-27 14:58:18,498 - INFO - Scale [4] Iter [38000/100000] Loss_D: -0.0003 Loss_G: 0.4459 (Adv: 0.4458 Rec: 0.0001)
2025-07-27 14:58:57,644 - INFO - Scale [4] Iter [39000/100000] Loss_D: 0.0036 Loss_G: 3.4880 (Adv: 3.4875 Rec: 0.0005)
2025-07-27 14:59:36,187 - INFO - Scale [4] Iter [40000/100000] Loss_D: 0.0028 Loss_G: 3.3173 (Adv: 3.3168 Rec: 0.0005)
2025-07-27 15:00:15,149 - INFO - Scale [4] Iter [41000/100000] Loss_D: 0.0014 Loss_G: 3.4610 (Adv: 3.4604 Rec: 0.0006)
2025-07-27 15:00:53,991 - INFO - Scale [4] Iter [42000/100000] Loss_D: 0.0010 Loss_G: 3.3216 (Adv: 3.3214 Rec: 0.0002)
2025-07-27 15:01:33,338 - INFO - Scale [4] Iter [43000/100000] Loss_D: 0.0003 Loss_G: 3.3441 (Adv: 3.3440 Rec: 0.0001)
2025-07-27 15:02:12,407 - INFO - Scale [4] Iter [44000/100000] Loss_D: 0.0003 Loss_G: 2.9391 (Adv: 2.9386 Rec: 0.0004)
2025-07-27 15:02:51,610 - INFO - Scale [4] Iter [45000/100000] Loss_D: 0.0001 Loss_G: 3.2674 (Adv: 3.2662 Rec: 0.0012)
2025-07-27 15:03:30,756 - INFO - Scale [4] Iter [46000/100000] Loss_D: 0.0001 Loss_G: 3.2345 (Adv: 3.2343 Rec: 0.0002)
2025-07-27 15:04:09,759 - INFO - Scale [4] Iter [47000/100000] Loss_D: 0.0011 Loss_G: 3.2838 (Adv: 3.2838 Rec: 0.0001)
2025-07-27 15:04:48,625 - INFO - Scale [4] Iter [48000/100000] Loss_D: 0.0028 Loss_G: -1.7219 (Adv: -1.7221 Rec: 0.0002)
2025-07-27 15:05:27,580 - INFO - Scale [4] Iter [49000/100000] Loss_D: 0.0013 Loss_G: -1.5316 (Adv: -1.5318 Rec: 0.0003)
2025-07-27 15:06:06,883 - INFO - Scale [4] Iter [50000/100000] Loss_D: 0.0005 Loss_G: -0.9016 (Adv: -0.9018 Rec: 0.0002)
2025-07-27 15:06:45,994 - INFO - Scale [4] Iter [51000/100000] Loss_D: 0.0006 Loss_G: -0.9924 (Adv: -0.9924 Rec: 0.0000)
2025-07-27 15:07:24,988 - INFO - Scale [4] Iter [52000/100000] Loss_D: 0.0004 Loss_G: -0.9817 (Adv: -0.9817 Rec: 0.0000)
2025-07-27 15:08:04,193 - INFO - Scale [4] Iter [53000/100000] Loss_D: 0.0008 Loss_G: -0.9752 (Adv: -0.9752 Rec: 0.0000)
2025-07-27 15:08:43,417 - INFO - Scale [4] Iter [54000/100000] Loss_D: 0.0000 Loss_G: -0.8527 (Adv: -0.8528 Rec: 0.0000)
2025-07-27 15:09:22,403 - INFO - Scale [4] Iter [55000/100000] Loss_D: -0.0000 Loss_G: -0.6632 (Adv: -0.6633 Rec: 0.0000)
2025-07-27 15:10:01,420 - INFO - Scale [4] Iter [56000/100000] Loss_D: -0.0001 Loss_G: -0.5293 (Adv: -0.5294 Rec: 0.0000)
2025-07-27 15:10:40,349 - INFO - Scale [4] Iter [57000/100000] Loss_D: -0.0002 Loss_G: -0.5387 (Adv: -0.5387 Rec: 0.0000)
2025-07-27 15:11:19,495 - INFO - Scale [4] Iter [58000/100000] Loss_D: -0.0003 Loss_G: -0.5668 (Adv: -0.5669 Rec: 0.0000)
2025-07-27 15:11:58,845 - INFO - Scale [4] Iter [59000/100000] Loss_D: 0.0003 Loss_G: -0.6775 (Adv: -0.6775 Rec: 0.0000)
2025-07-27 15:12:37,662 - INFO - Scale [4] Iter [60000/100000] Loss_D: -0.0002 Loss_G: -0.4587 (Adv: -0.4587 Rec: 0.0000)
2025-07-27 15:13:16,902 - INFO - Scale [4] Iter [61000/100000] Loss_D: 0.0002 Loss_G: -0.3688 (Adv: -0.3688 Rec: 0.0000)
2025-07-27 15:13:55,734 - INFO - Scale [4] Iter [62000/100000] Loss_D: -0.0003 Loss_G: -0.4781 (Adv: -0.4782 Rec: 0.0000)
2025-07-27 15:14:34,546 - INFO - Scale [4] Iter [63000/100000] Loss_D: -0.0002 Loss_G: -0.3314 (Adv: -0.3314 Rec: 0.0000)
2025-07-27 15:15:13,892 - INFO - Scale [4] Iter [64000/100000] Loss_D: 0.0000 Loss_G: -0.5091 (Adv: -0.5091 Rec: 0.0000)
2025-07-27 15:15:53,006 - INFO - Scale [4] Iter [65000/100000] Loss_D: 0.0004 Loss_G: -0.2687 (Adv: -0.2687 Rec: 0.0000)
2025-07-27 15:16:32,163 - INFO - Scale [4] Iter [66000/100000] Loss_D: -0.0002 Loss_G: -0.4124 (Adv: -0.4124 Rec: 0.0000)
2025-07-27 15:17:11,437 - INFO - Scale [4] Iter [67000/100000] Loss_D: -0.0001 Loss_G: -0.1475 (Adv: -0.1476 Rec: 0.0000)
2025-07-27 15:17:50,417 - INFO - Scale [4] Iter [68000/100000] Loss_D: 0.0000 Loss_G: -0.1006 (Adv: -0.1006 Rec: 0.0000)
2025-07-27 15:18:29,704 - INFO - Scale [4] Iter [69000/100000] Loss_D: -0.0002 Loss_G: -0.0858 (Adv: -0.0858 Rec: 0.0000)
2025-07-27 15:19:08,663 - INFO - Scale [4] Iter [70000/100000] Loss_D: 0.0004 Loss_G: -0.0732 (Adv: -0.0732 Rec: 0.0000)
2025-07-27 15:19:47,642 - INFO - Scale [4] Iter [71000/100000] Loss_D: -0.0003 Loss_G: 0.0116 (Adv: 0.0115 Rec: 0.0000)
2025-07-27 15:20:26,907 - INFO - Scale [4] Iter [72000/100000] Loss_D: -0.0003 Loss_G: 0.0568 (Adv: 0.0568 Rec: 0.0000)
2025-07-27 15:21:05,932 - INFO - Scale [4] Iter [73000/100000] Loss_D: -0.0006 Loss_G: 0.2862 (Adv: 0.2862 Rec: 0.0000)
2025-07-27 15:21:44,864 - INFO - Scale [4] Iter [74000/100000] Loss_D: -0.0004 Loss_G: 0.3544 (Adv: 0.3544 Rec: 0.0000)
2025-07-27 15:22:24,064 - INFO - Scale [4] Iter [75000/100000] Loss_D: -0.0003 Loss_G: 0.1490 (Adv: 0.1490 Rec: 0.0000)
2025-07-27 15:23:02,941 - INFO - Scale [4] Iter [76000/100000] Loss_D: -0.0004 Loss_G: 0.2919 (Adv: 0.2918 Rec: 0.0000)
2025-07-27 15:23:42,071 - INFO - Scale [4] Iter [77000/100000] Loss_D: -0.0002 Loss_G: 0.3219 (Adv: 0.3219 Rec: 0.0000)
2025-07-27 15:24:21,031 - INFO - Scale [4] Iter [78000/100000] Loss_D: -0.0003 Loss_G: 0.2715 (Adv: 0.2715 Rec: 0.0000)
2025-07-27 15:24:59,912 - INFO - Scale [4] Iter [79000/100000] Loss_D: -0.0001 Loss_G: 0.2956 (Adv: 0.2956 Rec: 0.0000)
2025-07-27 15:25:39,391 - INFO - Scale [4] Iter [80000/100000] Loss_D: -0.0004 Loss_G: 0.3296 (Adv: 0.3296 Rec: 0.0000)
2025-07-27 15:26:18,103 - INFO - Scale [4] Iter [81000/100000] Loss_D: -0.0003 Loss_G: 0.3781 (Adv: 0.3781 Rec: 0.0000)
2025-07-27 15:26:57,087 - INFO - Scale [4] Iter [82000/100000] Loss_D: -0.0003 Loss_G: 0.3393 (Adv: 0.3393 Rec: 0.0000)
2025-07-27 15:27:35,989 - INFO - Scale [4] Iter [83000/100000] Loss_D: -0.0002 Loss_G: 0.3517 (Adv: 0.3517 Rec: 0.0000)
2025-07-27 15:28:14,891 - INFO - Scale [4] Iter [84000/100000] Loss_D: -0.0003 Loss_G: 0.3563 (Adv: 0.3563 Rec: 0.0000)
2025-07-27 15:28:53,819 - INFO - Scale [4] Iter [85000/100000] Loss_D: -0.0004 Loss_G: 0.3713 (Adv: 0.3713 Rec: 0.0000)
2025-07-27 15:29:32,795 - INFO - Scale [4] Iter [86000/100000] Loss_D: -0.0003 Loss_G: 0.4255 (Adv: 0.4255 Rec: 0.0000)
2025-07-27 15:30:12,148 - INFO - Scale [4] Iter [87000/100000] Loss_D: -0.0005 Loss_G: 0.4776 (Adv: 0.4775 Rec: 0.0000)
2025-07-27 15:30:51,087 - INFO - Scale [4] Iter [88000/100000] Loss_D: -0.0004 Loss_G: 0.4570 (Adv: 0.4570 Rec: 0.0000)
2025-07-27 15:31:30,010 - INFO - Scale [4] Iter [89000/100000] Loss_D: -0.0004 Loss_G: 0.4561 (Adv: 0.4561 Rec: 0.0000)
2025-07-27 15:32:09,225 - INFO - Scale [4] Iter [90000/100000] Loss_D: -0.0005 Loss_G: 0.4634 (Adv: 0.4634 Rec: 0.0000)
2025-07-27 15:32:48,366 - INFO - Scale [4] Iter [91000/100000] Loss_D: -0.0003 Loss_G: 0.4397 (Adv: 0.4397 Rec: 0.0000)
2025-07-27 15:33:27,304 - INFO - Scale [4] Iter [92000/100000] Loss_D: -0.0005 Loss_G: 0.4218 (Adv: 0.4218 Rec: 0.0000)
2025-07-27 15:34:06,562 - INFO - Scale [4] Iter [93000/100000] Loss_D: -0.0005 Loss_G: 0.4931 (Adv: 0.4931 Rec: 0.0000)
2025-07-27 15:34:45,424 - INFO - Scale [4] Iter [94000/100000] Loss_D: -0.0005 Loss_G: 0.4834 (Adv: 0.4834 Rec: 0.0000)
2025-07-27 15:35:24,683 - INFO - Scale [4] Iter [95000/100000] Loss_D: -0.0005 Loss_G: 0.4892 (Adv: 0.4892 Rec: 0.0000)
2025-07-27 15:36:03,678 - INFO - Scale [4] Iter [96000/100000] Loss_D: -0.0004 Loss_G: 0.4841 (Adv: 0.4841 Rec: 0.0000)
2025-07-27 15:36:42,715 - INFO - Scale [4] Iter [97000/100000] Loss_D: -0.0006 Loss_G: 0.4852 (Adv: 0.4852 Rec: 0.0000)
2025-07-27 15:37:21,413 - INFO - Scale [4] Iter [98000/100000] Loss_D: -0.0004 Loss_G: 0.4481 (Adv: 0.4481 Rec: 0.0000)
2025-07-27 15:38:00,502 - INFO - Scale [4] Iter [99000/100000] Loss_D: 0.0000 Loss_G: 0.5076 (Adv: 0.5076 Rec: 0.0000)
2025-07-27 15:38:39,828 - INFO - 
--- Training Scale 3 ---
2025-07-27 15:38:39,829 - INFO - Volume size: torch.Size([10, 20, 20])
2025-07-27 15:38:39,946 - INFO - Scale [3] Iter [0/100000] Loss_D: 467.4598 Loss_G: 2.5002 (Adv: 0.0956 Rec: 2.4046)
2025-07-27 15:39:32,841 - INFO - Scale [3] Iter [1000/100000] Loss_D: 0.0008 Loss_G: -0.0489 (Adv: -0.0513 Rec: 0.0024)
2025-07-27 15:40:25,661 - INFO - Scale [3] Iter [2000/100000] Loss_D: -0.0003 Loss_G: 0.0261 (Adv: 0.0252 Rec: 0.0009)
2025-07-27 15:41:18,457 - INFO - Scale [3] Iter [3000/100000] Loss_D: -0.0007 Loss_G: 0.0133 (Adv: 0.0124 Rec: 0.0009)
2025-07-27 15:42:11,309 - INFO - Scale [3] Iter [4000/100000] Loss_D: 0.0301 Loss_G: 0.6142 (Adv: 0.6104 Rec: 0.0038)
2025-07-27 15:43:04,310 - INFO - Scale [3] Iter [5000/100000] Loss_D: 0.0076 Loss_G: 0.3743 (Adv: 0.3733 Rec: 0.0010)
2025-07-27 15:43:58,283 - INFO - Scale [3] Iter [6000/100000] Loss_D: 0.0026 Loss_G: 0.5621 (Adv: 0.5567 Rec: 0.0054)
2025-07-27 15:44:51,345 - INFO - Scale [3] Iter [7000/100000] Loss_D: -0.0004 Loss_G: 0.5743 (Adv: 0.5726 Rec: 0.0017)
2025-07-27 15:45:44,161 - INFO - Scale [3] Iter [8000/100000] Loss_D: -0.0004 Loss_G: 0.4559 (Adv: 0.4553 Rec: 0.0007)
2025-07-27 15:46:36,926 - INFO - Scale [3] Iter [9000/100000] Loss_D: 0.0155 Loss_G: -0.9093 (Adv: -0.9117 Rec: 0.0024)
2025-07-27 15:47:29,837 - INFO - Scale [3] Iter [10000/100000] Loss_D: 0.0004 Loss_G: -0.7118 (Adv: -0.7132 Rec: 0.0014)
2025-07-27 15:48:22,656 - INFO - Scale [3] Iter [11000/100000] Loss_D: -0.0001 Loss_G: -0.7768 (Adv: -0.7800 Rec: 0.0031)
2025-07-27 15:49:15,485 - INFO - Scale [3] Iter [12000/100000] Loss_D: 0.0001 Loss_G: -0.8714 (Adv: -0.8719 Rec: 0.0005)
2025-07-27 15:50:08,309 - INFO - Scale [3] Iter [13000/100000] Loss_D: 0.0027 Loss_G: -0.5153 (Adv: -0.5159 Rec: 0.0006)
2025-07-27 15:51:01,105 - INFO - Scale [3] Iter [14000/100000] Loss_D: 0.0028 Loss_G: -0.4377 (Adv: -0.4387 Rec: 0.0010)
2025-07-27 15:51:54,030 - INFO - Scale [3] Iter [15000/100000] Loss_D: -0.0002 Loss_G: -0.3517 (Adv: -0.3521 Rec: 0.0004)
2025-07-27 15:52:46,839 - INFO - Scale [3] Iter [16000/100000] Loss_D: 0.0007 Loss_G: 0.0063 (Adv: 0.0054 Rec: 0.0009)
2025-07-27 15:53:39,646 - INFO - Scale [3] Iter [17000/100000] Loss_D: 0.0009 Loss_G: -0.1430 (Adv: -0.1446 Rec: 0.0016)
2025-07-27 15:54:32,465 - INFO - Scale [3] Iter [18000/100000] Loss_D: -0.0005 Loss_G: -0.2189 (Adv: -0.2193 Rec: 0.0003)
2025-07-27 15:55:25,281 - INFO - Scale [3] Iter [19000/100000] Loss_D: -0.0005 Loss_G: -0.1945 (Adv: -0.1950 Rec: 0.0005)
2025-07-27 15:56:18,181 - INFO - Scale [3] Iter [20000/100000] Loss_D: 0.0000 Loss_G: -0.2366 (Adv: -0.2374 Rec: 0.0008)
2025-07-27 15:57:11,009 - INFO - Scale [3] Iter [21000/100000] Loss_D: -0.0001 Loss_G: 0.4935 (Adv: 0.4931 Rec: 0.0004)
2025-07-27 15:58:03,851 - INFO - Scale [3] Iter [22000/100000] Loss_D: -0.0000 Loss_G: 0.5866 (Adv: 0.5848 Rec: 0.0018)
2025-07-27 15:58:56,661 - INFO - Scale [3] Iter [23000/100000] Loss_D: 0.0000 Loss_G: 0.6516 (Adv: 0.6502 Rec: 0.0013)
2025-07-27 15:59:49,469 - INFO - Scale [3] Iter [24000/100000] Loss_D: 0.0000 Loss_G: 0.7135 (Adv: 0.7131 Rec: 0.0005)
2025-07-27 16:00:42,173 - INFO - Scale [3] Iter [25000/100000] Loss_D: 0.0002 Loss_G: 0.9154 (Adv: 0.9150 Rec: 0.0003)
2025-07-27 16:01:34,940 - INFO - Scale [3] Iter [26000/100000] Loss_D: 0.0168 Loss_G: -6.2024 (Adv: -6.2135 Rec: 0.0111)
2025-07-27 16:02:27,758 - INFO - Scale [3] Iter [27000/100000] Loss_D: -0.0001 Loss_G: -5.9339 (Adv: -5.9345 Rec: 0.0005)
2025-07-27 16:03:20,575 - INFO - Scale [3] Iter [28000/100000] Loss_D: 0.0003 Loss_G: -5.8760 (Adv: -5.8761 Rec: 0.0001)
2025-07-27 16:04:13,423 - INFO - Scale [3] Iter [29000/100000] Loss_D: 0.0002 Loss_G: -5.7632 (Adv: -5.7638 Rec: 0.0006)
2025-07-27 16:05:06,344 - INFO - Scale [3] Iter [30000/100000] Loss_D: 0.0000 Loss_G: -5.6437 (Adv: -5.6439 Rec: 0.0002)
2025-07-27 16:05:59,155 - INFO - Scale [3] Iter [31000/100000] Loss_D: -0.0000 Loss_G: -5.6509 (Adv: -5.6512 Rec: 0.0003)
2025-07-27 16:06:52,024 - INFO - Scale [3] Iter [32000/100000] Loss_D: -0.0001 Loss_G: -5.6003 (Adv: -5.6006 Rec: 0.0004)
2025-07-27 16:07:44,854 - INFO - Scale [3] Iter [33000/100000] Loss_D: -0.0002 Loss_G: -5.6346 (Adv: -5.6349 Rec: 0.0003)
2025-07-27 16:08:37,675 - INFO - Scale [3] Iter [34000/100000] Loss_D: 0.0015 Loss_G: 3.3824 (Adv: 3.3817 Rec: 0.0007)
2025-07-27 16:09:30,598 - INFO - Scale [3] Iter [35000/100000] Loss_D: 0.0021 Loss_G: 3.5738 (Adv: 3.5713 Rec: 0.0024)
2025-07-27 16:10:23,421 - INFO - Scale [3] Iter [36000/100000] Loss_D: 0.0019 Loss_G: 3.4987 (Adv: 3.4974 Rec: 0.0013)
2025-07-27 16:11:16,243 - INFO - Scale [3] Iter [37000/100000] Loss_D: 0.0010 Loss_G: 3.5670 (Adv: 3.5662 Rec: 0.0008)
2025-07-27 16:12:09,086 - INFO - Scale [3] Iter [38000/100000] Loss_D: 0.0005 Loss_G: 3.9180 (Adv: 3.9177 Rec: 0.0003)
2025-07-27 16:13:01,901 - INFO - Scale [3] Iter [39000/100000] Loss_D: 0.0001 Loss_G: 4.2800 (Adv: 4.2797 Rec: 0.0003)
2025-07-27 16:13:54,805 - INFO - Scale [3] Iter [40000/100000] Loss_D: 0.0006 Loss_G: 4.6196 (Adv: 4.6193 Rec: 0.0003)
2025-07-27 16:14:47,629 - INFO - Scale [3] Iter [41000/100000] Loss_D: -0.0002 Loss_G: 4.2999 (Adv: 4.2997 Rec: 0.0002)
2025-07-27 16:15:40,450 - INFO - Scale [3] Iter [42000/100000] Loss_D: -0.0001 Loss_G: 4.3026 (Adv: 4.3023 Rec: 0.0003)
2025-07-27 16:16:33,264 - INFO - Scale [3] Iter [43000/100000] Loss_D: -0.0001 Loss_G: 4.5777 (Adv: 4.5773 Rec: 0.0004)
2025-07-27 16:17:26,085 - INFO - Scale [3] Iter [44000/100000] Loss_D: -0.0002 Loss_G: 4.3625 (Adv: 4.3622 Rec: 0.0002)
2025-07-27 16:18:19,011 - INFO - Scale [3] Iter [45000/100000] Loss_D: 0.0002 Loss_G: -9.2826 (Adv: -9.2828 Rec: 0.0002)
2025-07-27 16:19:11,856 - INFO - Scale [3] Iter [46000/100000] Loss_D: 0.0020 Loss_G: -9.2487 (Adv: -9.2490 Rec: 0.0004)
2025-07-27 16:20:04,681 - INFO - Scale [3] Iter [47000/100000] Loss_D: 0.0015 Loss_G: -8.8505 (Adv: -8.8519 Rec: 0.0015)
2025-07-27 16:20:57,476 - INFO - Scale [3] Iter [48000/100000] Loss_D: 0.0015 Loss_G: -9.0174 (Adv: -9.0176 Rec: 0.0003)
2025-07-27 16:21:50,343 - INFO - Scale [3] Iter [49000/100000] Loss_D: 0.0006 Loss_G: -9.0238 (Adv: -9.0241 Rec: 0.0002)
2025-07-27 16:22:43,257 - INFO - Scale [3] Iter [50000/100000] Loss_D: 0.0002 Loss_G: -8.6234 (Adv: -8.6235 Rec: 0.0001)
2025-07-27 16:23:36,073 - INFO - Scale [3] Iter [51000/100000] Loss_D: 0.0004 Loss_G: -8.6372 (Adv: -8.6372 Rec: 0.0000)
2025-07-27 16:24:28,872 - INFO - Scale [3] Iter [52000/100000] Loss_D: 0.0002 Loss_G: -8.6130 (Adv: -8.6131 Rec: 0.0000)
2025-07-27 16:25:21,695 - INFO - Scale [3] Iter [53000/100000] Loss_D: 0.0000 Loss_G: -8.4781 (Adv: -8.4781 Rec: 0.0000)
2025-07-27 16:26:14,517 - INFO - Scale [3] Iter [54000/100000] Loss_D: 0.0002 Loss_G: -8.2319 (Adv: -8.2319 Rec: 0.0000)
2025-07-27 16:27:07,430 - INFO - Scale [3] Iter [55000/100000] Loss_D: -0.0003 Loss_G: -7.9889 (Adv: -7.9889 Rec: 0.0000)
2025-07-27 16:28:00,256 - INFO - Scale [3] Iter [56000/100000] Loss_D: -0.0001 Loss_G: -7.4472 (Adv: -7.4472 Rec: 0.0000)
2025-07-27 16:28:53,047 - INFO - Scale [3] Iter [57000/100000] Loss_D: -0.0000 Loss_G: -7.1667 (Adv: -7.1668 Rec: 0.0000)
2025-07-27 16:29:45,878 - INFO - Scale [3] Iter [58000/100000] Loss_D: -0.0001 Loss_G: -6.9858 (Adv: -6.9858 Rec: 0.0000)
2025-07-27 16:30:38,683 - INFO - Scale [3] Iter [59000/100000] Loss_D: -0.0002 Loss_G: -7.0682 (Adv: -7.0682 Rec: 0.0000)
2025-07-27 16:31:31,596 - INFO - Scale [3] Iter [60000/100000] Loss_D: -0.0002 Loss_G: -7.1799 (Adv: -7.1799 Rec: 0.0000)
2025-07-27 16:32:24,394 - INFO - Scale [3] Iter [61000/100000] Loss_D: 0.0005 Loss_G: -6.8896 (Adv: -6.8896 Rec: 0.0000)
2025-07-27 16:33:17,242 - INFO - Scale [3] Iter [62000/100000] Loss_D: -0.0004 Loss_G: -6.7534 (Adv: -6.7534 Rec: 0.0000)
2025-07-27 16:34:10,065 - INFO - Scale [3] Iter [63000/100000] Loss_D: -0.0004 Loss_G: -6.8331 (Adv: -6.8331 Rec: 0.0000)
2025-07-27 16:35:02,866 - INFO - Scale [3] Iter [64000/100000] Loss_D: -0.0003 Loss_G: -7.1081 (Adv: -7.1081 Rec: 0.0000)
2025-07-27 16:35:55,777 - INFO - Scale [3] Iter [65000/100000] Loss_D: -0.0003 Loss_G: -7.0033 (Adv: -7.0033 Rec: 0.0000)
2025-07-27 16:36:48,624 - INFO - Scale [3] Iter [66000/100000] Loss_D: -0.0002 Loss_G: -6.8880 (Adv: -6.8880 Rec: 0.0000)
2025-07-27 16:37:41,445 - INFO - Scale [3] Iter [67000/100000] Loss_D: 0.0000 Loss_G: -6.8156 (Adv: -6.8156 Rec: 0.0000)
2025-07-27 16:38:34,255 - INFO - Scale [3] Iter [68000/100000] Loss_D: -0.0005 Loss_G: -6.9026 (Adv: -6.9026 Rec: 0.0000)
2025-07-27 16:39:27,091 - INFO - Scale [3] Iter [69000/100000] Loss_D: -0.0005 Loss_G: -6.9196 (Adv: -6.9196 Rec: 0.0000)
2025-07-27 16:40:19,999 - INFO - Scale [3] Iter [70000/100000] Loss_D: -0.0003 Loss_G: -7.0310 (Adv: -7.0310 Rec: 0.0000)
2025-07-27 16:41:12,831 - INFO - Scale [3] Iter [71000/100000] Loss_D: -0.0005 Loss_G: -6.9507 (Adv: -6.9507 Rec: 0.0000)
2025-07-27 16:42:05,647 - INFO - Scale [3] Iter [72000/100000] Loss_D: -0.0000 Loss_G: -7.0965 (Adv: -7.0965 Rec: 0.0000)
2025-07-27 16:42:58,440 - INFO - Scale [3] Iter [73000/100000] Loss_D: -0.0004 Loss_G: -7.2212 (Adv: -7.2212 Rec: 0.0000)
2025-07-27 16:43:51,287 - INFO - Scale [3] Iter [74000/100000] Loss_D: -0.0003 Loss_G: -7.1476 (Adv: -7.1476 Rec: 0.0000)
2025-07-27 16:44:44,174 - INFO - Scale [3] Iter [75000/100000] Loss_D: -0.0003 Loss_G: -7.1982 (Adv: -7.1982 Rec: 0.0000)
2025-07-27 16:45:37,016 - INFO - Scale [3] Iter [76000/100000] Loss_D: -0.0004 Loss_G: -7.2401 (Adv: -7.2401 Rec: 0.0000)
2025-07-27 16:46:29,793 - INFO - Scale [3] Iter [77000/100000] Loss_D: -0.0006 Loss_G: -7.2460 (Adv: -7.2460 Rec: 0.0000)
2025-07-27 16:47:22,633 - INFO - Scale [3] Iter [78000/100000] Loss_D: -0.0005 Loss_G: -7.2766 (Adv: -7.2766 Rec: 0.0000)
2025-07-27 16:48:15,474 - INFO - Scale [3] Iter [79000/100000] Loss_D: -0.0002 Loss_G: -7.2848 (Adv: -7.2848 Rec: 0.0000)
2025-07-27 16:49:08,387 - INFO - Scale [3] Iter [80000/100000] Loss_D: -0.0005 Loss_G: -7.2875 (Adv: -7.2875 Rec: 0.0000)
2025-07-27 16:50:01,224 - INFO - Scale [3] Iter [81000/100000] Loss_D: -0.0006 Loss_G: -7.2767 (Adv: -7.2767 Rec: 0.0000)
2025-07-27 16:50:54,037 - INFO - Scale [3] Iter [82000/100000] Loss_D: -0.0006 Loss_G: -7.3711 (Adv: -7.3711 Rec: 0.0000)
2025-07-27 16:51:46,886 - INFO - Scale [3] Iter [83000/100000] Loss_D: -0.0005 Loss_G: -7.3468 (Adv: -7.3468 Rec: 0.0000)
2025-07-27 16:52:39,712 - INFO - Scale [3] Iter [84000/100000] Loss_D: -0.0006 Loss_G: -7.3456 (Adv: -7.3456 Rec: 0.0000)
2025-07-27 16:53:32,636 - INFO - Scale [3] Iter [85000/100000] Loss_D: -0.0007 Loss_G: -7.3948 (Adv: -7.3948 Rec: 0.0000)
2025-07-27 16:54:25,458 - INFO - Scale [3] Iter [86000/100000] Loss_D: -0.0007 Loss_G: -7.4078 (Adv: -7.4078 Rec: 0.0000)
2025-07-27 16:55:18,279 - INFO - Scale [3] Iter [87000/100000] Loss_D: -0.0005 Loss_G: -7.4103 (Adv: -7.4103 Rec: 0.0000)
2025-07-27 16:56:11,110 - INFO - Scale [3] Iter [88000/100000] Loss_D: -0.0001 Loss_G: -7.4110 (Adv: -7.4110 Rec: 0.0000)
2025-07-27 16:57:03,918 - INFO - Scale [3] Iter [89000/100000] Loss_D: -0.0007 Loss_G: -7.4492 (Adv: -7.4492 Rec: 0.0000)
2025-07-27 16:57:56,837 - INFO - Scale [3] Iter [90000/100000] Loss_D: -0.0005 Loss_G: -7.4756 (Adv: -7.4756 Rec: 0.0000)
2025-07-27 16:58:49,621 - INFO - Scale [3] Iter [91000/100000] Loss_D: -0.0007 Loss_G: -7.5008 (Adv: -7.5008 Rec: 0.0000)
2025-07-27 16:59:42,434 - INFO - Scale [3] Iter [92000/100000] Loss_D: -0.0007 Loss_G: -7.5338 (Adv: -7.5338 Rec: 0.0000)
2025-07-27 17:00:35,262 - INFO - Scale [3] Iter [93000/100000] Loss_D: -0.0005 Loss_G: -7.5577 (Adv: -7.5577 Rec: 0.0000)
2025-07-27 17:01:28,067 - INFO - Scale [3] Iter [94000/100000] Loss_D: -0.0004 Loss_G: -7.5595 (Adv: -7.5595 Rec: 0.0000)
2025-07-27 17:02:21,008 - INFO - Scale [3] Iter [95000/100000] Loss_D: -0.0007 Loss_G: -7.5000 (Adv: -7.5000 Rec: 0.0000)
2025-07-27 17:03:13,848 - INFO - Scale [3] Iter [96000/100000] Loss_D: -0.0005 Loss_G: -7.5105 (Adv: -7.5105 Rec: 0.0000)
2025-07-27 17:04:06,653 - INFO - Scale [3] Iter [97000/100000] Loss_D: -0.0006 Loss_G: -7.4881 (Adv: -7.4881 Rec: 0.0000)
2025-07-27 17:04:59,445 - INFO - Scale [3] Iter [98000/100000] Loss_D: -0.0007 Loss_G: -7.4642 (Adv: -7.4642 Rec: 0.0000)
2025-07-27 17:05:52,248 - INFO - Scale [3] Iter [99000/100000] Loss_D: -0.0006 Loss_G: -7.5135 (Adv: -7.5135 Rec: 0.0000)
2025-07-27 17:06:45,211 - INFO - 
--- Training Scale 2 ---
2025-07-27 17:06:45,211 - INFO - Volume size: torch.Size([12, 23, 23])
2025-07-27 17:06:45,359 - INFO - Scale [2] Iter [0/100000] Loss_D: 505.6647 Loss_G: 2.2594 (Adv: 0.6173 Rec: 1.6421)
2025-07-27 17:08:04,679 - INFO - Scale [2] Iter [1000/100000] Loss_D: -0.0001 Loss_G: 0.0319 (Adv: 0.0278 Rec: 0.0041)
2025-07-27 17:09:23,911 - INFO - Scale [2] Iter [2000/100000] Loss_D: -0.0005 Loss_G: 0.0108 (Adv: 0.0099 Rec: 0.0010)
2025-07-27 17:10:43,176 - INFO - Scale [2] Iter [3000/100000] Loss_D: -0.0008 Loss_G: 0.0315 (Adv: 0.0302 Rec: 0.0013)
2025-07-27 17:12:02,419 - INFO - Scale [2] Iter [4000/100000] Loss_D: 0.0736 Loss_G: 0.6052 (Adv: 0.5975 Rec: 0.0077)
2025-07-27 17:13:21,747 - INFO - Scale [2] Iter [5000/100000] Loss_D: 0.0051 Loss_G: 0.7664 (Adv: 0.7630 Rec: 0.0034)
2025-07-27 17:14:40,996 - INFO - Scale [2] Iter [6000/100000] Loss_D: 0.0021 Loss_G: 0.7737 (Adv: 0.7719 Rec: 0.0018)
2025-07-27 17:16:00,239 - INFO - Scale [2] Iter [7000/100000] Loss_D: 0.0000 Loss_G: 0.6426 (Adv: 0.6410 Rec: 0.0016)
2025-07-27 17:17:19,469 - INFO - Scale [2] Iter [8000/100000] Loss_D: -0.0001 Loss_G: 0.6076 (Adv: 0.6068 Rec: 0.0008)
2025-07-27 17:18:38,722 - INFO - Scale [2] Iter [9000/100000] Loss_D: 0.4762 Loss_G: 1.4263 (Adv: 1.4206 Rec: 0.0057)
2025-07-27 17:19:58,056 - INFO - Scale [2] Iter [10000/100000] Loss_D: 0.0228 Loss_G: 1.6782 (Adv: 1.6771 Rec: 0.0011)
2025-07-27 17:21:17,309 - INFO - Scale [2] Iter [11000/100000] Loss_D: 0.0124 Loss_G: 1.4043 (Adv: 1.4032 Rec: 0.0011)
2025-07-27 17:22:36,575 - INFO - Scale [2] Iter [12000/100000] Loss_D: 0.0003 Loss_G: 1.2219 (Adv: 1.2211 Rec: 0.0008)
2025-07-27 17:23:55,848 - INFO - Scale [2] Iter [13000/100000] Loss_D: -0.0000 Loss_G: 1.1340 (Adv: 1.1330 Rec: 0.0010)
2025-07-27 17:25:15,115 - INFO - Scale [2] Iter [14000/100000] Loss_D: 0.1580 Loss_G: 0.4860 (Adv: 0.4454 Rec: 0.0406)
2025-07-27 17:26:34,443 - INFO - Scale [2] Iter [15000/100000] Loss_D: 0.0018 Loss_G: 0.6893 (Adv: 0.6871 Rec: 0.0022)
2025-07-27 17:27:53,709 - INFO - Scale [2] Iter [16000/100000] Loss_D: 0.0001 Loss_G: 0.6829 (Adv: 0.6816 Rec: 0.0013)
2025-07-27 17:29:12,956 - INFO - Scale [2] Iter [17000/100000] Loss_D: -0.0001 Loss_G: 0.6415 (Adv: 0.6399 Rec: 0.0016)
2025-07-27 17:30:32,200 - INFO - Scale [2] Iter [18000/100000] Loss_D: -0.0002 Loss_G: 0.6251 (Adv: 0.6246 Rec: 0.0005)
2025-07-27 17:31:51,439 - INFO - Scale [2] Iter [19000/100000] Loss_D: 0.1635 Loss_G: 0.9911 (Adv: 0.9885 Rec: 0.0026)
2025-07-27 17:33:10,770 - INFO - Scale [2] Iter [20000/100000] Loss_D: 0.0006 Loss_G: 0.6806 (Adv: 0.6801 Rec: 0.0005)
2025-07-27 17:34:30,001 - INFO - Scale [2] Iter [21000/100000] Loss_D: 0.0009 Loss_G: 0.5126 (Adv: 0.5117 Rec: 0.0008)
2025-07-27 17:35:49,252 - INFO - Scale [2] Iter [22000/100000] Loss_D: -0.0000 Loss_G: 0.3548 (Adv: 0.3547 Rec: 0.0001)
2025-07-27 17:37:08,540 - INFO - Scale [2] Iter [23000/100000] Loss_D: -0.0000 Loss_G: 0.2615 (Adv: 0.2612 Rec: 0.0003)
2025-07-27 17:38:27,772 - INFO - Scale [2] Iter [24000/100000] Loss_D: 0.0553 Loss_G: 1.5010 (Adv: 1.5006 Rec: 0.0004)
2025-07-27 17:39:47,100 - INFO - Scale [2] Iter [25000/100000] Loss_D: 0.0047 Loss_G: 1.4953 (Adv: 1.4944 Rec: 0.0009)
2025-07-27 17:41:06,340 - INFO - Scale [2] Iter [26000/100000] Loss_D: 0.0005 Loss_G: 0.6667 (Adv: 0.6661 Rec: 0.0006)
2025-07-27 17:42:25,577 - INFO - Scale [2] Iter [27000/100000] Loss_D: -0.0000 Loss_G: 0.4897 (Adv: 0.4891 Rec: 0.0006)
2025-07-27 17:43:44,818 - INFO - Scale [2] Iter [28000/100000] Loss_D: 0.0066 Loss_G: 3.6141 (Adv: 3.6139 Rec: 0.0002)
2025-07-27 17:45:04,028 - INFO - Scale [2] Iter [29000/100000] Loss_D: 0.0006 Loss_G: 2.7562 (Adv: 2.7541 Rec: 0.0021)
2025-07-27 17:46:23,325 - INFO - Scale [2] Iter [30000/100000] Loss_D: -0.0001 Loss_G: 2.6512 (Adv: 2.6506 Rec: 0.0006)
2025-07-27 17:47:42,567 - INFO - Scale [2] Iter [31000/100000] Loss_D: -0.0002 Loss_G: 2.5675 (Adv: 2.5674 Rec: 0.0001)
2025-07-27 17:49:01,805 - INFO - Scale [2] Iter [32000/100000] Loss_D: 0.0207 Loss_G: -0.6658 (Adv: -0.6659 Rec: 0.0001)
2025-07-27 17:50:21,033 - INFO - Scale [2] Iter [33000/100000] Loss_D: 0.0027 Loss_G: -0.6239 (Adv: -0.6240 Rec: 0.0001)
2025-07-27 17:51:40,300 - INFO - Scale [2] Iter [34000/100000] Loss_D: -0.0000 Loss_G: -0.5294 (Adv: -0.5297 Rec: 0.0002)
2025-07-27 17:52:59,620 - INFO - Scale [2] Iter [35000/100000] Loss_D: -0.0002 Loss_G: -0.5093 (Adv: -0.5095 Rec: 0.0003)
2025-07-27 17:54:18,856 - INFO - Scale [2] Iter [36000/100000] Loss_D: 0.0024 Loss_G: -0.4574 (Adv: -0.4578 Rec: 0.0004)
2025-07-27 17:55:38,095 - INFO - Scale [2] Iter [37000/100000] Loss_D: -0.0000 Loss_G: -0.0651 (Adv: -0.0654 Rec: 0.0003)
2025-07-27 17:56:57,357 - INFO - Scale [2] Iter [38000/100000] Loss_D: -0.0000 Loss_G: -0.0927 (Adv: -0.0928 Rec: 0.0001)
2025-07-27 17:58:16,609 - INFO - Scale [2] Iter [39000/100000] Loss_D: -0.0000 Loss_G: -0.1198 (Adv: -0.1201 Rec: 0.0003)
2025-07-27 17:59:35,950 - INFO - Scale [2] Iter [40000/100000] Loss_D: 0.0001 Loss_G: -0.1366 (Adv: -0.1368 Rec: 0.0002)
2025-07-27 18:00:55,201 - INFO - Scale [2] Iter [41000/100000] Loss_D: 0.0001 Loss_G: -0.1012 (Adv: -0.1014 Rec: 0.0002)
2025-07-27 18:02:14,436 - INFO - Scale [2] Iter [42000/100000] Loss_D: 0.0001 Loss_G: 1.9140 (Adv: 1.9138 Rec: 0.0002)
2025-07-27 18:03:33,689 - INFO - Scale [2] Iter [43000/100000] Loss_D: 0.0003 Loss_G: 1.9360 (Adv: 1.9358 Rec: 0.0002)
2025-07-27 18:04:52,956 - INFO - Scale [2] Iter [44000/100000] Loss_D: 0.0005 Loss_G: 1.9120 (Adv: 1.9119 Rec: 0.0001)
2025-07-27 18:06:12,277 - INFO - Scale [2] Iter [45000/100000] Loss_D: 0.0002 Loss_G: 1.8484 (Adv: 1.8480 Rec: 0.0004)
2025-07-27 18:07:31,529 - INFO - Scale [2] Iter [46000/100000] Loss_D: 0.0008 Loss_G: -3.2557 (Adv: -3.2561 Rec: 0.0003)
2025-07-27 18:08:50,774 - INFO - Scale [2] Iter [47000/100000] Loss_D: 0.0012 Loss_G: -3.0348 (Adv: -3.0353 Rec: 0.0005)
2025-07-27 18:10:10,023 - INFO - Scale [2] Iter [48000/100000] Loss_D: 0.0013 Loss_G: -2.8504 (Adv: -2.8511 Rec: 0.0007)
2025-07-27 18:11:29,255 - INFO - Scale [2] Iter [49000/100000] Loss_D: 0.0012 Loss_G: -2.7287 (Adv: -2.7295 Rec: 0.0008)
2025-07-27 18:12:48,585 - INFO - Scale [2] Iter [50000/100000] Loss_D: 0.0003 Loss_G: 0.0939 (Adv: 0.0937 Rec: 0.0001)
2025-07-27 18:14:07,836 - INFO - Scale [2] Iter [51000/100000] Loss_D: 0.0005 Loss_G: 0.0693 (Adv: 0.0692 Rec: 0.0001)
2025-07-27 18:15:27,102 - INFO - Scale [2] Iter [52000/100000] Loss_D: 0.0009 Loss_G: 0.0718 (Adv: 0.0718 Rec: 0.0001)
2025-07-27 18:16:46,337 - INFO - Scale [2] Iter [53000/100000] Loss_D: 0.0011 Loss_G: 0.1654 (Adv: 0.1654 Rec: 0.0001)
2025-07-27 18:18:05,566 - INFO - Scale [2] Iter [54000/100000] Loss_D: 0.0015 Loss_G: 0.2838 (Adv: 0.2838 Rec: 0.0000)
2025-07-27 18:19:24,900 - INFO - Scale [2] Iter [55000/100000] Loss_D: 0.0013 Loss_G: 0.4657 (Adv: 0.4656 Rec: 0.0001)
2025-07-27 18:20:44,148 - INFO - Scale [2] Iter [56000/100000] Loss_D: 0.0008 Loss_G: 0.4452 (Adv: 0.4452 Rec: 0.0001)
2025-07-27 18:22:03,412 - INFO - Scale [2] Iter [57000/100000] Loss_D: 0.0007 Loss_G: 0.3770 (Adv: 0.3769 Rec: 0.0000)
2025-07-27 18:23:22,649 - INFO - Scale [2] Iter [58000/100000] Loss_D: 0.0007 Loss_G: -0.0049 (Adv: -0.0050 Rec: 0.0000)
2025-07-27 18:24:41,903 - INFO - Scale [2] Iter [59000/100000] Loss_D: 0.0002 Loss_G: -0.3197 (Adv: -0.3197 Rec: 0.0000)
2025-07-27 18:26:01,235 - INFO - Scale [2] Iter [60000/100000] Loss_D: 0.0001 Loss_G: -0.2501 (Adv: -0.2501 Rec: 0.0000)
2025-07-27 18:27:20,481 - INFO - Scale [2] Iter [61000/100000] Loss_D: -0.0001 Loss_G: -0.2517 (Adv: -0.2517 Rec: 0.0000)
2025-07-27 18:28:39,732 - INFO - Scale [2] Iter [62000/100000] Loss_D: 0.0001 Loss_G: -0.4996 (Adv: -0.4997 Rec: 0.0000)
2025-07-27 18:29:58,995 - INFO - Scale [2] Iter [63000/100000] Loss_D: 0.0000 Loss_G: -0.5658 (Adv: -0.5659 Rec: 0.0000)
2025-07-27 18:31:18,233 - INFO - Scale [2] Iter [64000/100000] Loss_D: 0.0001 Loss_G: -0.4026 (Adv: -0.4027 Rec: 0.0000)
2025-07-27 18:32:37,560 - INFO - Scale [2] Iter [65000/100000] Loss_D: -0.0001 Loss_G: -0.4903 (Adv: -0.4904 Rec: 0.0000)
2025-07-27 18:33:56,820 - INFO - Scale [2] Iter [66000/100000] Loss_D: -0.0001 Loss_G: -0.3221 (Adv: -0.3221 Rec: 0.0000)
2025-07-27 18:35:16,074 - INFO - Scale [2] Iter [67000/100000] Loss_D: -0.0003 Loss_G: -0.1117 (Adv: -0.1117 Rec: 0.0000)
2025-07-27 18:36:35,336 - INFO - Scale [2] Iter [68000/100000] Loss_D: -0.0003 Loss_G: -0.2636 (Adv: -0.2636 Rec: 0.0000)
2025-07-27 18:37:54,591 - INFO - Scale [2] Iter [69000/100000] Loss_D: 0.0002 Loss_G: -0.0628 (Adv: -0.0629 Rec: 0.0000)
2025-07-27 18:39:13,907 - INFO - Scale [2] Iter [70000/100000] Loss_D: -0.0002 Loss_G: -0.2695 (Adv: -0.2695 Rec: 0.0000)
2025-07-27 18:40:33,142 - INFO - Scale [2] Iter [71000/100000] Loss_D: -0.0001 Loss_G: -0.4109 (Adv: -0.4110 Rec: 0.0000)
2025-07-27 18:41:52,386 - INFO - Scale [2] Iter [72000/100000] Loss_D: -0.0001 Loss_G: -0.2953 (Adv: -0.2953 Rec: 0.0000)
2025-07-27 18:43:11,618 - INFO - Scale [2] Iter [73000/100000] Loss_D: -0.0001 Loss_G: -0.3615 (Adv: -0.3615 Rec: 0.0000)
2025-07-27 18:44:30,840 - INFO - Scale [2] Iter [74000/100000] Loss_D: -0.0003 Loss_G: -0.0892 (Adv: -0.0892 Rec: 0.0000)
2025-07-27 18:45:50,156 - INFO - Scale [2] Iter [75000/100000] Loss_D: -0.0001 Loss_G: 0.1686 (Adv: 0.1686 Rec: 0.0000)
2025-07-27 18:47:09,409 - INFO - Scale [2] Iter [76000/100000] Loss_D: -0.0003 Loss_G: 0.1595 (Adv: 0.1595 Rec: 0.0000)
2025-07-27 18:48:28,639 - INFO - Scale [2] Iter [77000/100000] Loss_D: -0.0006 Loss_G: 0.2231 (Adv: 0.2231 Rec: 0.0000)
2025-07-27 18:49:47,875 - INFO - Scale [2] Iter [78000/100000] Loss_D: -0.0005 Loss_G: 0.2429 (Adv: 0.2429 Rec: 0.0000)
2025-07-27 18:51:07,114 - INFO - Scale [2] Iter [79000/100000] Loss_D: -0.0004 Loss_G: 0.2495 (Adv: 0.2495 Rec: 0.0000)
2025-07-27 18:52:26,448 - INFO - Scale [2] Iter [80000/100000] Loss_D: -0.0005 Loss_G: 0.2866 (Adv: 0.2866 Rec: 0.0000)
2025-07-27 18:53:45,695 - INFO - Scale [2] Iter [81000/100000] Loss_D: -0.0005 Loss_G: 0.3001 (Adv: 0.3001 Rec: 0.0000)
2025-07-27 18:55:04,956 - INFO - Scale [2] Iter [82000/100000] Loss_D: -0.0004 Loss_G: 0.3307 (Adv: 0.3307 Rec: 0.0000)
2025-07-27 18:56:24,183 - INFO - Scale [2] Iter [83000/100000] Loss_D: -0.0005 Loss_G: 0.4072 (Adv: 0.4072 Rec: 0.0000)
2025-07-27 18:57:43,422 - INFO - Scale [2] Iter [84000/100000] Loss_D: -0.0005 Loss_G: 0.4537 (Adv: 0.4537 Rec: 0.0000)
2025-07-27 18:59:02,748 - INFO - Scale [2] Iter [85000/100000] Loss_D: -0.0005 Loss_G: 0.4337 (Adv: 0.4337 Rec: 0.0000)
2025-07-27 19:00:21,999 - INFO - Scale [2] Iter [86000/100000] Loss_D: -0.0005 Loss_G: 0.4168 (Adv: 0.4168 Rec: 0.0000)
2025-07-27 19:01:41,266 - INFO - Scale [2] Iter [87000/100000] Loss_D: -0.0005 Loss_G: 0.4748 (Adv: 0.4748 Rec: 0.0000)
2025-07-27 19:03:00,506 - INFO - Scale [2] Iter [88000/100000] Loss_D: -0.0005 Loss_G: 0.4890 (Adv: 0.4890 Rec: 0.0000)
2025-07-27 19:04:19,752 - INFO - Scale [2] Iter [89000/100000] Loss_D: -0.0005 Loss_G: 0.4530 (Adv: 0.4530 Rec: 0.0000)
2025-07-27 19:05:39,096 - INFO - Scale [2] Iter [90000/100000] Loss_D: -0.0005 Loss_G: 0.4666 (Adv: 0.4666 Rec: 0.0000)
2025-07-27 19:06:58,362 - INFO - Scale [2] Iter [91000/100000] Loss_D: -0.0003 Loss_G: 0.5039 (Adv: 0.5039 Rec: 0.0000)
2025-07-27 19:08:17,602 - INFO - Scale [2] Iter [92000/100000] Loss_D: -0.0005 Loss_G: 0.4898 (Adv: 0.4897 Rec: 0.0000)
2025-07-27 19:09:36,854 - INFO - Scale [2] Iter [93000/100000] Loss_D: -0.0002 Loss_G: 0.5178 (Adv: 0.5178 Rec: 0.0000)
2025-07-27 19:10:56,111 - INFO - Scale [2] Iter [94000/100000] Loss_D: -0.0003 Loss_G: 0.5710 (Adv: 0.5710 Rec: 0.0000)
2025-07-27 19:12:15,434 - INFO - Scale [2] Iter [95000/100000] Loss_D: -0.0005 Loss_G: 0.5672 (Adv: 0.5672 Rec: 0.0000)
2025-07-27 19:13:34,652 - INFO - Scale [2] Iter [96000/100000] Loss_D: -0.0005 Loss_G: 0.6141 (Adv: 0.6141 Rec: 0.0000)
2025-07-27 19:14:53,897 - INFO - Scale [2] Iter [97000/100000] Loss_D: -0.0004 Loss_G: 0.6101 (Adv: 0.6101 Rec: 0.0000)
2025-07-27 19:16:13,148 - INFO - Scale [2] Iter [98000/100000] Loss_D: -0.0005 Loss_G: 0.6238 (Adv: 0.6238 Rec: 0.0000)
2025-07-27 19:17:32,391 - INFO - Scale [2] Iter [99000/100000] Loss_D: -0.0005 Loss_G: 0.6472 (Adv: 0.6472 Rec: 0.0000)
2025-07-27 19:18:51,928 - INFO - 
--- Training Scale 1 ---
2025-07-27 19:18:51,928 - INFO - Volume size: torch.Size([14, 27, 27])
2025-07-27 19:18:52,104 - INFO - Scale [1] Iter [0/100000] Loss_D: 931.9367 Loss_G: 2.1859 (Adv: -0.5640 Rec: 2.7500)
2025-07-27 19:20:50,394 - INFO - Scale [1] Iter [1000/100000] Loss_D: 0.0016 Loss_G: 0.0974 (Adv: 0.0756 Rec: 0.0218)
2025-07-27 19:22:48,694 - INFO - Scale [1] Iter [2000/100000] Loss_D: 0.3264 Loss_G: -0.0598 (Adv: -0.0737 Rec: 0.0139)
2025-07-27 19:24:46,925 - INFO - Scale [1] Iter [3000/100000] Loss_D: 0.0300 Loss_G: 0.0397 (Adv: 0.0313 Rec: 0.0084)
2025-07-27 19:26:45,144 - INFO - Scale [1] Iter [4000/100000] Loss_D: 0.0038 Loss_G: -0.0100 (Adv: -0.0204 Rec: 0.0103)
2025-07-27 19:28:43,440 - INFO - Scale [1] Iter [5000/100000] Loss_D: 0.0007 Loss_G: 0.0733 (Adv: 0.0597 Rec: 0.0136)
2025-07-27 19:30:41,644 - INFO - Scale [1] Iter [6000/100000] Loss_D: 0.0004 Loss_G: 0.0620 (Adv: 0.0583 Rec: 0.0037)
2025-07-27 19:32:39,844 - INFO - Scale [1] Iter [7000/100000] Loss_D: -0.0002 Loss_G: 0.0494 (Adv: 0.0458 Rec: 0.0036)
2025-07-27 19:34:38,042 - INFO - Scale [1] Iter [8000/100000] Loss_D: 0.0644 Loss_G: 1.0482 (Adv: 1.0257 Rec: 0.0225)
2025-07-27 19:36:36,256 - INFO - Scale [1] Iter [9000/100000] Loss_D: 0.0094 Loss_G: 1.0759 (Adv: 1.0660 Rec: 0.0099)
2025-07-27 19:38:34,571 - INFO - Scale [1] Iter [10000/100000] Loss_D: 0.0007 Loss_G: 0.0941 (Adv: 0.0781 Rec: 0.0160)
2025-07-27 19:40:32,777 - INFO - Scale [1] Iter [11000/100000] Loss_D: -0.0000 Loss_G: -0.1100 (Adv: -0.1148 Rec: 0.0048)
2025-07-27 19:42:31,011 - INFO - Scale [1] Iter [12000/100000] Loss_D: 0.1720 Loss_G: -0.3015 (Adv: -0.3248 Rec: 0.0233)
2025-07-27 19:44:29,222 - INFO - Scale [1] Iter [13000/100000] Loss_D: 0.0078 Loss_G: 0.1751 (Adv: 0.1701 Rec: 0.0051)
2025-07-27 19:46:27,432 - INFO - Scale [1] Iter [14000/100000] Loss_D: 0.0006 Loss_G: 0.4502 (Adv: 0.4462 Rec: 0.0040)
2025-07-27 19:48:26,919 - INFO - Scale [1] Iter [15000/100000] Loss_D: -0.0001 Loss_G: 0.5549 (Adv: 0.5518 Rec: 0.0031)
2025-07-27 19:50:28,464 - INFO - Scale [1] Iter [16000/100000] Loss_D: -0.0002 Loss_G: 0.5977 (Adv: 0.5923 Rec: 0.0054)
2025-07-27 19:52:29,538 - INFO - Scale [1] Iter [17000/100000] Loss_D: 0.1519 Loss_G: 1.3182 (Adv: 1.3127 Rec: 0.0055)
2025-07-27 19:54:29,249 - INFO - Scale [1] Iter [18000/100000] Loss_D: 0.0002 Loss_G: 1.0988 (Adv: 1.0950 Rec: 0.0038)
2025-07-27 19:56:28,439 - INFO - Scale [1] Iter [19000/100000] Loss_D: 0.0002 Loss_G: 1.1658 (Adv: 1.1627 Rec: 0.0031)
2025-07-27 19:58:28,189 - INFO - Scale [1] Iter [20000/100000] Loss_D: 0.0001 Loss_G: 1.1612 (Adv: 1.1586 Rec: 0.0027)
2025-07-27 20:00:27,624 - INFO - Scale [1] Iter [21000/100000] Loss_D: 0.0332 Loss_G: 0.6545 (Adv: 0.6519 Rec: 0.0026)
2025-07-27 20:02:28,627 - INFO - Scale [1] Iter [22000/100000] Loss_D: 0.0076 Loss_G: 0.8343 (Adv: 0.8319 Rec: 0.0024)
2025-07-27 20:04:30,212 - INFO - Scale [1] Iter [23000/100000] Loss_D: -0.0001 Loss_G: 0.9508 (Adv: 0.9483 Rec: 0.0025)
2025-07-27 20:06:30,835 - INFO - Scale [1] Iter [24000/100000] Loss_D: -0.0002 Loss_G: 1.0142 (Adv: 1.0105 Rec: 0.0036)
2025-07-27 20:08:31,058 - INFO - Scale [1] Iter [25000/100000] Loss_D: 0.0641 Loss_G: -0.1818 (Adv: -0.1846 Rec: 0.0027)
2025-07-27 20:10:30,915 - INFO - Scale [1] Iter [26000/100000] Loss_D: 0.0020 Loss_G: 0.1242 (Adv: 0.1180 Rec: 0.0062)
2025-07-27 20:12:29,704 - INFO - Scale [1] Iter [27000/100000] Loss_D: -0.0001 Loss_G: 0.2747 (Adv: 0.2705 Rec: 0.0041)
2025-07-27 20:14:29,302 - INFO - Scale [1] Iter [28000/100000] Loss_D: -0.0001 Loss_G: 0.2842 (Adv: 0.2805 Rec: 0.0037)
2025-07-27 20:16:31,073 - INFO - Scale [1] Iter [29000/100000] Loss_D: 0.0367 Loss_G: -0.5433 (Adv: -0.5464 Rec: 0.0031)
2025-07-27 20:18:33,107 - INFO - Scale [1] Iter [30000/100000] Loss_D: -0.0001 Loss_G: -0.4531 (Adv: -0.4554 Rec: 0.0023)
2025-07-27 20:20:34,804 - INFO - Scale [1] Iter [31000/100000] Loss_D: -0.0001 Loss_G: -0.4156 (Adv: -0.4178 Rec: 0.0023)
2025-07-27 20:22:35,888 - INFO - Scale [1] Iter [32000/100000] Loss_D: -0.0001 Loss_G: -0.3856 (Adv: -0.3892 Rec: 0.0036)
2025-07-27 20:24:37,509 - INFO - Scale [1] Iter [33000/100000] Loss_D: -0.0000 Loss_G: -0.3106 (Adv: -0.3140 Rec: 0.0033)
2025-07-27 20:26:36,498 - INFO - Scale [1] Iter [34000/100000] Loss_D: 0.0074 Loss_G: 0.3245 (Adv: 0.3221 Rec: 0.0023)
2025-07-27 20:28:34,955 - INFO - Scale [1] Iter [35000/100000] Loss_D: 0.0000 Loss_G: 0.2253 (Adv: 0.2224 Rec: 0.0029)
2025-07-27 20:30:33,289 - INFO - Scale [1] Iter [36000/100000] Loss_D: -0.0001 Loss_G: 0.1315 (Adv: 0.1291 Rec: 0.0024)
2025-07-27 20:32:31,638 - INFO - Scale [1] Iter [37000/100000] Loss_D: -0.0001 Loss_G: 0.1012 (Adv: 0.0985 Rec: 0.0026)
2025-07-27 20:34:29,994 - INFO - Scale [1] Iter [38000/100000] Loss_D: -0.0002 Loss_G: 0.1189 (Adv: 0.1169 Rec: 0.0020)
2025-07-27 20:36:28,325 - INFO - Scale [1] Iter [39000/100000] Loss_D: 0.0005 Loss_G: -0.6600 (Adv: -0.6626 Rec: 0.0027)
2025-07-27 20:38:26,789 - INFO - Scale [1] Iter [40000/100000] Loss_D: -0.0001 Loss_G: -0.8985 (Adv: -0.9012 Rec: 0.0027)
2025-07-27 20:40:25,118 - INFO - Scale [1] Iter [41000/100000] Loss_D: -0.0002 Loss_G: -0.9012 (Adv: -0.9032 Rec: 0.0020)
2025-07-27 20:42:23,605 - INFO - Scale [1] Iter [42000/100000] Loss_D: -0.0002 Loss_G: -0.8799 (Adv: -0.8825 Rec: 0.0026)
2025-07-27 20:44:22,089 - INFO - Scale [1] Iter [43000/100000] Loss_D: 0.1454 Loss_G: -1.8978 (Adv: -1.8998 Rec: 0.0020)
2025-07-27 20:46:20,305 - INFO - Scale [1] Iter [44000/100000] Loss_D: 0.0034 Loss_G: -2.2767 (Adv: -2.2792 Rec: 0.0025)
2025-07-27 20:48:18,588 - INFO - Scale [1] Iter [45000/100000] Loss_D: -0.0002 Loss_G: -2.4187 (Adv: -2.4213 Rec: 0.0025)
2025-07-27 20:50:16,754 - INFO - Scale [1] Iter [46000/100000] Loss_D: -0.0001 Loss_G: -2.4684 (Adv: -2.4711 Rec: 0.0027)
2025-07-27 20:52:14,955 - INFO - Scale [1] Iter [47000/100000] Loss_D: 0.0295 Loss_G: -1.8128 (Adv: -1.8148 Rec: 0.0020)
2025-07-27 20:54:13,120 - INFO - Scale [1] Iter [48000/100000] Loss_D: 0.0001 Loss_G: -1.6585 (Adv: -1.6608 Rec: 0.0022)
2025-07-27 20:56:11,291 - INFO - Scale [1] Iter [49000/100000] Loss_D: -0.0000 Loss_G: -1.6532 (Adv: -1.6553 Rec: 0.0021)
2025-07-27 20:58:09,654 - INFO - Scale [1] Iter [50000/100000] Loss_D: -0.0002 Loss_G: -1.6505 (Adv: -1.6526 Rec: 0.0021)
2025-07-27 21:00:07,920 - INFO - Scale [1] Iter [51000/100000] Loss_D: -0.0001 Loss_G: -1.6378 (Adv: -1.6398 Rec: 0.0020)
2025-07-27 21:02:06,205 - INFO - Scale [1] Iter [52000/100000] Loss_D: -0.0001 Loss_G: -1.6327 (Adv: -1.6346 Rec: 0.0020)
2025-07-27 21:04:04,470 - INFO - Scale [1] Iter [53000/100000] Loss_D: -0.0001 Loss_G: -1.6430 (Adv: -1.6450 Rec: 0.0020)
2025-07-27 21:06:02,770 - INFO - Scale [1] Iter [54000/100000] Loss_D: -0.0000 Loss_G: -1.6646 (Adv: -1.6666 Rec: 0.0020)
2025-07-27 21:08:01,139 - INFO - Scale [1] Iter [55000/100000] Loss_D: -0.0001 Loss_G: -1.6535 (Adv: -1.6555 Rec: 0.0020)
2025-07-27 21:09:59,421 - INFO - Scale [1] Iter [56000/100000] Loss_D: -0.0001 Loss_G: -1.6508 (Adv: -1.6527 Rec: 0.0020)
2025-07-27 21:11:57,704 - INFO - Scale [1] Iter [57000/100000] Loss_D: 0.0000 Loss_G: -1.6191 (Adv: -1.6211 Rec: 0.0020)
2025-07-27 21:13:55,972 - INFO - Scale [1] Iter [58000/100000] Loss_D: -0.0002 Loss_G: -1.5460 (Adv: -1.5480 Rec: 0.0020)
2025-07-27 21:15:54,239 - INFO - Scale [1] Iter [59000/100000] Loss_D: -0.0000 Loss_G: -1.4770 (Adv: -1.4790 Rec: 0.0020)
2025-07-27 21:17:52,588 - INFO - Scale [1] Iter [60000/100000] Loss_D: -0.0002 Loss_G: -1.4432 (Adv: -1.4451 Rec: 0.0019)
2025-07-27 21:19:50,870 - INFO - Scale [1] Iter [61000/100000] Loss_D: -0.0002 Loss_G: -1.4700 (Adv: -1.4720 Rec: 0.0019)
2025-07-27 21:21:49,170 - INFO - Scale [1] Iter [62000/100000] Loss_D: -0.0003 Loss_G: -1.5742 (Adv: -1.5762 Rec: 0.0019)
2025-07-27 21:23:47,458 - INFO - Scale [1] Iter [63000/100000] Loss_D: -0.0005 Loss_G: -1.4975 (Adv: -1.4994 Rec: 0.0020)
2025-07-27 21:25:45,731 - INFO - Scale [1] Iter [64000/100000] Loss_D: -0.0001 Loss_G: -1.3719 (Adv: -1.3738 Rec: 0.0019)
2025-07-27 21:27:44,055 - INFO - Scale [1] Iter [65000/100000] Loss_D: -0.0003 Loss_G: -1.4296 (Adv: -1.4315 Rec: 0.0019)
2025-07-27 21:29:42,325 - INFO - Scale [1] Iter [66000/100000] Loss_D: -0.0004 Loss_G: -1.6247 (Adv: -1.6267 Rec: 0.0020)
2025-07-27 21:31:40,607 - INFO - Scale [1] Iter [67000/100000] Loss_D: -0.0004 Loss_G: -1.5247 (Adv: -1.5267 Rec: 0.0019)
2025-07-27 21:33:38,870 - INFO - Scale [1] Iter [68000/100000] Loss_D: -0.0002 Loss_G: -1.2956 (Adv: -1.2975 Rec: 0.0020)
2025-07-27 21:35:37,138 - INFO - Scale [1] Iter [69000/100000] Loss_D: -0.0004 Loss_G: -1.2400 (Adv: -1.2419 Rec: 0.0020)
2025-07-27 21:37:35,421 - INFO - Scale [1] Iter [70000/100000] Loss_D: -0.0004 Loss_G: -1.3426 (Adv: -1.3445 Rec: 0.0019)
2025-07-27 21:39:33,588 - INFO - Scale [1] Iter [71000/100000] Loss_D: -0.0005 Loss_G: -1.4517 (Adv: -1.4537 Rec: 0.0019)
2025-07-27 21:41:31,788 - INFO - Scale [1] Iter [72000/100000] Loss_D: -0.0003 Loss_G: -1.5537 (Adv: -1.5557 Rec: 0.0020)
2025-07-27 21:43:29,955 - INFO - Scale [1] Iter [73000/100000] Loss_D: -0.0005 Loss_G: -1.6387 (Adv: -1.6406 Rec: 0.0020)
2025-07-27 21:45:28,138 - INFO - Scale [1] Iter [74000/100000] Loss_D: -0.0006 Loss_G: -1.6691 (Adv: -1.6711 Rec: 0.0020)
2025-07-27 21:47:26,405 - INFO - Scale [1] Iter [75000/100000] Loss_D: -0.0006 Loss_G: -1.6952 (Adv: -1.6971 Rec: 0.0019)
2025-07-27 21:49:24,570 - INFO - Scale [1] Iter [76000/100000] Loss_D: -0.0007 Loss_G: -1.6982 (Adv: -1.7002 Rec: 0.0019)
2025-07-27 21:51:22,755 - INFO - Scale [1] Iter [77000/100000] Loss_D: -0.0007 Loss_G: -1.6970 (Adv: -1.6989 Rec: 0.0019)
2025-07-27 21:53:20,954 - INFO - Scale [1] Iter [78000/100000] Loss_D: -0.0007 Loss_G: -1.7152 (Adv: -1.7172 Rec: 0.0019)
2025-07-27 21:55:19,139 - INFO - Scale [1] Iter [79000/100000] Loss_D: -0.0006 Loss_G: -1.7179 (Adv: -1.7199 Rec: 0.0019)
2025-07-27 21:57:17,388 - INFO - Scale [1] Iter [80000/100000] Loss_D: -0.0007 Loss_G: -1.7333 (Adv: -1.7352 Rec: 0.0019)
2025-07-27 21:59:15,555 - INFO - Scale [1] Iter [81000/100000] Loss_D: -0.0007 Loss_G: -1.7700 (Adv: -1.7719 Rec: 0.0019)
2025-07-27 22:01:13,739 - INFO - Scale [1] Iter [82000/100000] Loss_D: -0.0007 Loss_G: -1.7448 (Adv: -1.7467 Rec: 0.0019)
2025-07-27 22:03:11,988 - INFO - Scale [1] Iter [83000/100000] Loss_D: -0.0006 Loss_G: -1.7565 (Adv: -1.7585 Rec: 0.0019)
2025-07-27 22:05:10,188 - INFO - Scale [1] Iter [84000/100000] Loss_D: -0.0007 Loss_G: -1.7353 (Adv: -1.7372 Rec: 0.0019)
2025-07-27 22:07:08,455 - INFO - Scale [1] Iter [85000/100000] Loss_D: -0.0007 Loss_G: -1.7484 (Adv: -1.7503 Rec: 0.0019)
2025-07-27 22:09:06,604 - INFO - Scale [1] Iter [86000/100000] Loss_D: -0.0007 Loss_G: -1.7603 (Adv: -1.7623 Rec: 0.0019)
2025-07-27 22:11:05,228 - INFO - Scale [1] Iter [87000/100000] Loss_D: -0.0007 Loss_G: -1.7349 (Adv: -1.7368 Rec: 0.0019)
2025-07-27 22:13:04,255 - INFO - Scale [1] Iter [88000/100000] Loss_D: -0.0007 Loss_G: -1.7367 (Adv: -1.7386 Rec: 0.0019)
2025-07-27 22:15:02,301 - INFO - Scale [1] Iter [89000/100000] Loss_D: -0.0007 Loss_G: -1.7470 (Adv: -1.7489 Rec: 0.0019)
2025-07-27 22:17:00,571 - INFO - Scale [1] Iter [90000/100000] Loss_D: -0.0004 Loss_G: -1.7298 (Adv: -1.7318 Rec: 0.0019)
2025-07-27 22:18:58,774 - INFO - Scale [1] Iter [91000/100000] Loss_D: -0.0006 Loss_G: -1.7133 (Adv: -1.7152 Rec: 0.0019)
2025-07-27 22:20:56,971 - INFO - Scale [1] Iter [92000/100000] Loss_D: -0.0007 Loss_G: -1.6870 (Adv: -1.6889 Rec: 0.0019)
2025-07-27 22:22:55,194 - INFO - Scale [1] Iter [93000/100000] Loss_D: -0.0007 Loss_G: -1.6685 (Adv: -1.6704 Rec: 0.0019)
2025-07-27 22:24:53,389 - INFO - Scale [1] Iter [94000/100000] Loss_D: -0.0007 Loss_G: -1.6658 (Adv: -1.6677 Rec: 0.0019)
2025-07-27 22:26:53,035 - INFO - Scale [1] Iter [95000/100000] Loss_D: -0.0007 Loss_G: -1.6688 (Adv: -1.6708 Rec: 0.0019)
2025-07-27 22:28:52,118 - INFO - Scale [1] Iter [96000/100000] Loss_D: -0.0008 Loss_G: -1.6763 (Adv: -1.6782 Rec: 0.0019)
2025-07-27 22:30:51,968 - INFO - Scale [1] Iter [97000/100000] Loss_D: -0.0008 Loss_G: -1.6888 (Adv: -1.6908 Rec: 0.0019)
2025-07-27 22:32:53,634 - INFO - Scale [1] Iter [98000/100000] Loss_D: -0.0007 Loss_G: -1.6966 (Adv: -1.6985 Rec: 0.0019)
2025-07-27 22:34:54,355 - INFO - Scale [1] Iter [99000/100000] Loss_D: -0.0006 Loss_G: -1.6950 (Adv: -1.6969 Rec: 0.0019)
2025-07-27 22:36:54,138 - INFO - 
--- Training Scale 0 ---
2025-07-27 22:36:54,138 - INFO - Volume size: torch.Size([16, 32, 32])
2025-07-27 22:36:54,388 - INFO - Scale [0] Iter [0/100000] Loss_D: 1193.3311 Loss_G: 2.5157 (Adv: -0.1846 Rec: 2.7002)
2025-07-27 22:40:04,938 - INFO - Scale [0] Iter [1000/100000] Loss_D: 131.0378 Loss_G: 1.7295 (Adv: 1.6512 Rec: 0.0783)
2025-07-27 22:43:15,404 - INFO - Scale [0] Iter [2000/100000] Loss_D: 0.0559 Loss_G: -0.0033 (Adv: -0.0564 Rec: 0.0531)
2025-07-27 22:46:27,495 - INFO - Scale [0] Iter [3000/100000] Loss_D: 0.0020 Loss_G: 0.0433 (Adv: -0.0034 Rec: 0.0467)
2025-07-27 22:49:41,771 - INFO - Scale [0] Iter [4000/100000] Loss_D: -0.0001 Loss_G: 0.1314 (Adv: 0.0809 Rec: 0.0505)
2025-07-27 22:52:54,811 - INFO - Scale [0] Iter [5000/100000] Loss_D: -0.0000 Loss_G: 0.1124 (Adv: 0.0691 Rec: 0.0433)
2025-07-27 22:56:11,276 - INFO - Scale [0] Iter [6000/100000] Loss_D: 5.3241 Loss_G: 0.3144 (Adv: 0.2728 Rec: 0.0416)
2025-07-27 22:59:28,372 - INFO - Scale [0] Iter [7000/100000] Loss_D: 0.0790 Loss_G: 0.0529 (Adv: 0.0073 Rec: 0.0456)
2025-07-27 23:02:44,870 - INFO - Scale [0] Iter [8000/100000] Loss_D: 0.0049 Loss_G: 0.2528 (Adv: 0.2101 Rec: 0.0428)
2025-07-27 23:06:00,958 - INFO - Scale [0] Iter [9000/100000] Loss_D: 0.0013 Loss_G: 0.3854 (Adv: 0.3420 Rec: 0.0434)
2025-07-27 23:09:17,305 - INFO - Scale [0] Iter [10000/100000] Loss_D: -0.0002 Loss_G: 0.3022 (Adv: 0.2564 Rec: 0.0458)
2025-07-27 23:12:34,292 - INFO - Scale [0] Iter [11000/100000] Loss_D: 0.0847 Loss_G: 0.3075 (Adv: 0.2543 Rec: 0.0532)
2025-07-27 23:15:51,151 - INFO - Scale [0] Iter [12000/100000] Loss_D: 0.0050 Loss_G: -0.0046 (Adv: -0.0520 Rec: 0.0474)
2025-07-27 23:19:07,995 - INFO - Scale [0] Iter [13000/100000] Loss_D: 0.0004 Loss_G: 0.0788 (Adv: 0.0357 Rec: 0.0432)
2025-07-27 23:22:24,150 - INFO - Scale [0] Iter [14000/100000] Loss_D: -0.0002 Loss_G: 0.1336 (Adv: 0.0904 Rec: 0.0432)
2025-07-27 23:25:40,464 - INFO - Scale [0] Iter [15000/100000] Loss_D: 0.5520 Loss_G: -0.9430 (Adv: -0.9816 Rec: 0.0385)
2025-07-27 23:28:53,802 - INFO - Scale [0] Iter [16000/100000] Loss_D: 0.0396 Loss_G: -0.6841 (Adv: -0.7239 Rec: 0.0398)
2025-07-27 23:32:07,783 - INFO - Scale [0] Iter [17000/100000] Loss_D: 0.0011 Loss_G: -0.1252 (Adv: -0.1655 Rec: 0.0402)
2025-07-27 23:35:21,387 - INFO - Scale [0] Iter [18000/100000] Loss_D: 0.0058 Loss_G: -0.4246 (Adv: -0.4620 Rec: 0.0374)
2025-07-27 23:38:37,458 - INFO - Scale [0] Iter [19000/100000] Loss_D: 0.0003 Loss_G: -0.4492 (Adv: -0.4868 Rec: 0.0375)
2025-07-27 23:41:52,355 - INFO - Scale [0] Iter [20000/100000] Loss_D: -0.0001 Loss_G: -0.3345 (Adv: -0.3715 Rec: 0.0370)
2025-07-27 23:45:06,118 - INFO - Scale [0] Iter [21000/100000] Loss_D: 0.0000 Loss_G: -0.2679 (Adv: -0.3061 Rec: 0.0383)
2025-07-27 23:48:22,138 - INFO - Scale [0] Iter [22000/100000] Loss_D: 0.1919 Loss_G: -0.0994 (Adv: -0.1455 Rec: 0.0460)
2025-07-27 23:51:38,945 - INFO - Scale [0] Iter [23000/100000] Loss_D: 0.0049 Loss_G: 0.9652 (Adv: 0.9255 Rec: 0.0397)
2025-07-27 23:54:54,932 - INFO - Scale [0] Iter [24000/100000] Loss_D: 0.0000 Loss_G: 0.9818 (Adv: 0.9442 Rec: 0.0377)
2025-07-27 23:58:07,912 - INFO - Scale [0] Iter [25000/100000] Loss_D: -0.0001 Loss_G: 0.9240 (Adv: 0.8867 Rec: 0.0374)
2025-07-28 00:01:20,743 - INFO - Scale [0] Iter [26000/100000] Loss_D: -0.0003 Loss_G: 0.9422 (Adv: 0.9046 Rec: 0.0377)
2025-07-28 00:04:33,384 - INFO - Scale [0] Iter [27000/100000] Loss_D: 0.7168 Loss_G: -0.6382 (Adv: -0.6757 Rec: 0.0375)
2025-07-28 00:07:46,221 - INFO - Scale [0] Iter [28000/100000] Loss_D: 0.0072 Loss_G: 0.3145 (Adv: 0.2757 Rec: 0.0388)
2025-07-28 00:10:59,109 - INFO - Scale [0] Iter [29000/100000] Loss_D: -0.0001 Loss_G: 0.3296 (Adv: 0.2929 Rec: 0.0368)
2025-07-28 00:14:12,083 - INFO - Scale [0] Iter [30000/100000] Loss_D: -0.0003 Loss_G: 0.4176 (Adv: 0.3792 Rec: 0.0383)
2025-07-28 00:17:24,789 - INFO - Scale [0] Iter [31000/100000] Loss_D: -0.0004 Loss_G: 0.4075 (Adv: 0.3692 Rec: 0.0383)
2025-07-28 00:20:37,698 - INFO - Scale [0] Iter [32000/100000] Loss_D: 0.0599 Loss_G: 2.1102 (Adv: 2.0723 Rec: 0.0379)
2025-07-28 00:23:50,613 - INFO - Scale [0] Iter [33000/100000] Loss_D: 0.0016 Loss_G: 2.5116 (Adv: 2.4745 Rec: 0.0371)
2025-07-28 00:27:03,522 - INFO - Scale [0] Iter [34000/100000] Loss_D: 0.0001 Loss_G: 2.7231 (Adv: 2.6859 Rec: 0.0371)
2025-07-28 00:30:16,494 - INFO - Scale [0] Iter [35000/100000] Loss_D: -0.0002 Loss_G: 2.6803 (Adv: 2.6435 Rec: 0.0368)
2025-07-28 00:33:29,365 - INFO - Scale [0] Iter [36000/100000] Loss_D: -0.0003 Loss_G: 2.6915 (Adv: 2.6535 Rec: 0.0380)
2025-07-28 00:36:42,250 - INFO - Scale [0] Iter [37000/100000] Loss_D: 0.0103 Loss_G: -1.0278 (Adv: -1.0642 Rec: 0.0364)
2025-07-28 00:39:55,147 - INFO - Scale [0] Iter [38000/100000] Loss_D: 0.0004 Loss_G: -0.8885 (Adv: -0.9246 Rec: 0.0361)
2025-07-28 00:43:08,046 - INFO - Scale [0] Iter [39000/100000] Loss_D: -0.0001 Loss_G: -0.8021 (Adv: -0.8396 Rec: 0.0374)
2025-07-28 00:46:21,036 - INFO - Scale [0] Iter [40000/100000] Loss_D: -0.0002 Loss_G: -0.7353 (Adv: -0.7715 Rec: 0.0362)
2025-07-28 00:49:33,933 - INFO - Scale [0] Iter [41000/100000] Loss_D: -0.0002 Loss_G: -0.6883 (Adv: -0.7245 Rec: 0.0361)
2025-07-28 00:52:46,849 - INFO - Scale [0] Iter [42000/100000] Loss_D: 0.0091 Loss_G: -1.3091 (Adv: -1.3458 Rec: 0.0367)
2025-07-28 00:55:59,739 - INFO - Scale [0] Iter [43000/100000] Loss_D: -0.0000 Loss_G: -1.2771 (Adv: -1.3153 Rec: 0.0382)
2025-07-28 00:59:12,689 - INFO - Scale [0] Iter [44000/100000] Loss_D: -0.0001 Loss_G: -1.4642 (Adv: -1.5005 Rec: 0.0363)
2025-07-28 01:02:25,681 - INFO - Scale [0] Iter [45000/100000] Loss_D: -0.0003 Loss_G: -1.6941 (Adv: -1.7301 Rec: 0.0360)
2025-07-28 01:05:38,578 - INFO - Scale [0] Iter [46000/100000] Loss_D: -0.0004 Loss_G: -1.8470 (Adv: -1.8833 Rec: 0.0363)
2025-07-28 01:08:51,520 - INFO - Scale [0] Iter [47000/100000] Loss_D: -0.0004 Loss_G: -1.8019 (Adv: -1.8380 Rec: 0.0361)
2025-07-28 01:12:04,415 - INFO - Scale [0] Iter [48000/100000] Loss_D: 0.0059 Loss_G: -0.0238 (Adv: -0.0601 Rec: 0.0363)
2025-07-28 01:15:17,336 - INFO - Scale [0] Iter [49000/100000] Loss_D: -0.0001 Loss_G: -0.4767 (Adv: -0.5139 Rec: 0.0372)
2025-07-28 01:18:30,317 - INFO - Scale [0] Iter [50000/100000] Loss_D: -0.0002 Loss_G: -0.4823 (Adv: -0.5189 Rec: 0.0367)
2025-07-28 01:21:43,256 - INFO - Scale [0] Iter [51000/100000] Loss_D: -0.0002 Loss_G: -0.4856 (Adv: -0.5213 Rec: 0.0357)
2025-07-28 01:24:56,151 - INFO - Scale [0] Iter [52000/100000] Loss_D: -0.0002 Loss_G: -0.4886 (Adv: -0.5243 Rec: 0.0357)
2025-07-28 01:28:09,071 - INFO - Scale [0] Iter [53000/100000] Loss_D: -0.0002 Loss_G: -0.4897 (Adv: -0.5254 Rec: 0.0357)
2025-07-28 01:31:21,969 - INFO - Scale [0] Iter [54000/100000] Loss_D: -0.0002 Loss_G: -0.4978 (Adv: -0.5335 Rec: 0.0357)
2025-07-28 01:34:34,999 - INFO - Scale [0] Iter [55000/100000] Loss_D: -0.0002 Loss_G: -0.5491 (Adv: -0.5848 Rec: 0.0357)
2025-07-28 01:37:47,909 - INFO - Scale [0] Iter [56000/100000] Loss_D: -0.0003 Loss_G: -0.4968 (Adv: -0.5325 Rec: 0.0357)
2025-07-28 01:41:00,840 - INFO - Scale [0] Iter [57000/100000] Loss_D: -0.0003 Loss_G: -0.5121 (Adv: -0.5478 Rec: 0.0357)
2025-07-28 01:44:13,628 - INFO - Scale [0] Iter [58000/100000] Loss_D: -0.0004 Loss_G: -0.5250 (Adv: -0.5606 Rec: 0.0357)
2025-07-28 01:47:26,483 - INFO - Scale [0] Iter [59000/100000] Loss_D: -0.0002 Loss_G: -0.7047 (Adv: -0.7404 Rec: 0.0357)
2025-07-28 01:50:39,476 - INFO - Scale [0] Iter [60000/100000] Loss_D: -0.0003 Loss_G: -0.7417 (Adv: -0.7774 Rec: 0.0357)
2025-07-28 01:53:52,397 - INFO - Scale [0] Iter [61000/100000] Loss_D: -0.0004 Loss_G: -0.7489 (Adv: -0.7846 Rec: 0.0357)
2025-07-28 01:57:05,267 - INFO - Scale [0] Iter [62000/100000] Loss_D: -0.0006 Loss_G: -0.8747 (Adv: -0.9104 Rec: 0.0357)
2025-07-28 02:00:18,135 - INFO - Scale [0] Iter [63000/100000] Loss_D: -0.0008 Loss_G: -0.9384 (Adv: -0.9741 Rec: 0.0357)
2025-07-28 02:03:31,046 - INFO - Scale [0] Iter [64000/100000] Loss_D: -0.0002 Loss_G: -1.1123 (Adv: -1.1480 Rec: 0.0357)
2025-07-28 02:06:44,042 - INFO - Scale [0] Iter [65000/100000] Loss_D: -0.0004 Loss_G: -1.0945 (Adv: -1.1302 Rec: 0.0357)
2025-07-28 02:09:56,926 - INFO - Scale [0] Iter [66000/100000] Loss_D: -0.0006 Loss_G: -1.1713 (Adv: -1.2070 Rec: 0.0357)
2025-07-28 02:13:09,816 - INFO - Scale [0] Iter [67000/100000] Loss_D: -0.0007 Loss_G: -1.2073 (Adv: -1.2430 Rec: 0.0357)
2025-07-28 02:16:22,714 - INFO - Scale [0] Iter [68000/100000] Loss_D: -0.0008 Loss_G: -1.1767 (Adv: -1.2123 Rec: 0.0357)
2025-07-28 02:19:35,640 - INFO - Scale [0] Iter [69000/100000] Loss_D: 0.5926 Loss_G: -1.1057 (Adv: -1.1415 Rec: 0.0358)
2025-07-28 02:22:48,668 - INFO - Scale [0] Iter [70000/100000] Loss_D: -0.0005 Loss_G: -1.1988 (Adv: -1.2344 Rec: 0.0357)
2025-07-28 02:26:01,565 - INFO - Scale [0] Iter [71000/100000] Loss_D: -0.0005 Loss_G: -1.2101 (Adv: -1.2458 Rec: 0.0357)
2025-07-28 02:29:14,474 - INFO - Scale [0] Iter [72000/100000] Loss_D: -0.0007 Loss_G: -1.2213 (Adv: -1.2570 Rec: 0.0357)
2025-07-28 02:32:27,368 - INFO - Scale [0] Iter [73000/100000] Loss_D: -0.0008 Loss_G: -1.3073 (Adv: -1.3429 Rec: 0.0357)
2025-07-28 02:35:40,261 - INFO - Scale [0] Iter [74000/100000] Loss_D: -0.0008 Loss_G: -1.3889 (Adv: -1.4246 Rec: 0.0357)
2025-07-28 02:38:53,293 - INFO - Scale [0] Iter [75000/100000] Loss_D: -0.0005 Loss_G: -1.2324 (Adv: -1.2680 Rec: 0.0357)
2025-07-28 02:42:06,207 - INFO - Scale [0] Iter [76000/100000] Loss_D: -0.0005 Loss_G: -1.2393 (Adv: -1.2750 Rec: 0.0357)
2025-07-28 02:45:19,118 - INFO - Scale [0] Iter [77000/100000] Loss_D: -0.0006 Loss_G: -1.2389 (Adv: -1.2746 Rec: 0.0357)
2025-07-28 02:48:32,023 - INFO - Scale [0] Iter [78000/100000] Loss_D: -0.0006 Loss_G: -1.2610 (Adv: -1.2966 Rec: 0.0357)
2025-07-28 02:51:44,960 - INFO - Scale [0] Iter [79000/100000] Loss_D: -0.0006 Loss_G: -1.3022 (Adv: -1.3379 Rec: 0.0357)
2025-07-28 02:54:57,944 - INFO - Scale [0] Iter [80000/100000] Loss_D: -0.0007 Loss_G: -1.3517 (Adv: -1.3874 Rec: 0.0357)
2025-07-28 02:58:10,846 - INFO - Scale [0] Iter [81000/100000] Loss_D: -0.0007 Loss_G: -1.3950 (Adv: -1.4306 Rec: 0.0357)
2025-07-28 03:01:23,772 - INFO - Scale [0] Iter [82000/100000] Loss_D: -0.0008 Loss_G: -1.4306 (Adv: -1.4663 Rec: 0.0357)
2025-07-28 03:04:36,683 - INFO - Scale [0] Iter [83000/100000] Loss_D: -0.0008 Loss_G: -1.4706 (Adv: -1.5063 Rec: 0.0357)
2025-07-28 03:07:49,620 - INFO - Scale [0] Iter [84000/100000] Loss_D: -0.0008 Loss_G: -1.5085 (Adv: -1.5442 Rec: 0.0357)
2025-07-28 03:11:02,633 - INFO - Scale [0] Iter [85000/100000] Loss_D: -0.0009 Loss_G: -1.5293 (Adv: -1.5649 Rec: 0.0357)
2025-07-28 03:14:15,544 - INFO - Scale [0] Iter [86000/100000] Loss_D: -0.0009 Loss_G: -1.5451 (Adv: -1.5808 Rec: 0.0357)
2025-07-28 03:17:28,458 - INFO - Scale [0] Iter [87000/100000] Loss_D: -0.0009 Loss_G: -1.5241 (Adv: -1.5598 Rec: 0.0357)
2025-07-28 03:20:41,372 - INFO - Scale [0] Iter [88000/100000] Loss_D: -0.0009 Loss_G: -1.5557 (Adv: -1.5914 Rec: 0.0357)
2025-07-28 03:23:54,303 - INFO - Scale [0] Iter [89000/100000] Loss_D: -0.0002 Loss_G: -1.5836 (Adv: -1.6193 Rec: 0.0357)
2025-07-28 03:27:07,278 - INFO - Scale [0] Iter [90000/100000] Loss_D: -0.0008 Loss_G: -1.6068 (Adv: -1.6424 Rec: 0.0357)
2025-07-28 03:30:20,170 - INFO - Scale [0] Iter [91000/100000] Loss_D: -0.0009 Loss_G: -1.6337 (Adv: -1.6694 Rec: 0.0357)
2025-07-28 03:33:33,068 - INFO - Scale [0] Iter [92000/100000] Loss_D: -0.0010 Loss_G: -1.6412 (Adv: -1.6769 Rec: 0.0357)
2025-07-28 03:36:45,986 - INFO - Scale [0] Iter [93000/100000] Loss_D: -0.0010 Loss_G: -1.6521 (Adv: -1.6877 Rec: 0.0357)
2025-07-28 03:39:58,884 - INFO - Scale [0] Iter [94000/100000] Loss_D: -0.0009 Loss_G: -1.6600 (Adv: -1.6956 Rec: 0.0357)
2025-07-28 03:43:12,090 - INFO - Scale [0] Iter [95000/100000] Loss_D: -0.0009 Loss_G: -1.6817 (Adv: -1.7173 Rec: 0.0357)
2025-07-28 03:46:24,998 - INFO - Scale [0] Iter [96000/100000] Loss_D: -0.0010 Loss_G: -1.6989 (Adv: -1.7346 Rec: 0.0357)
2025-07-28 03:49:37,888 - INFO - Scale [0] Iter [97000/100000] Loss_D: -0.0010 Loss_G: -1.7005 (Adv: -1.7361 Rec: 0.0357)
2025-07-28 03:52:50,836 - INFO - Scale [0] Iter [98000/100000] Loss_D: -0.0010 Loss_G: -1.7167 (Adv: -1.7524 Rec: 0.0357)
2025-07-28 03:56:03,746 - INFO - Scale [0] Iter [99000/100000] Loss_D: -0.0011 Loss_G: -1.7350 (Adv: -1.7707 Rec: 0.0357)
2025-07-28 03:59:16,680 - INFO - 
--- Training Complete ---
2025-07-28 03:59:16,739 - INFO - Trained model saved to Channel_model_iter_100_000\trained_model_3d.pth
